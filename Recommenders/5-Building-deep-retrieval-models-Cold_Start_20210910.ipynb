{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T11:12:59.889579Z",
     "iopub.status.busy": "2021-08-14T11:12:59.888985Z",
     "iopub.status.idle": "2021-08-14T11:12:59.891460Z",
     "shell.execute_reply": "2021-08-14T11:12:59.891020Z"
    },
    "id": "uWqCArLO_kez"
   },
   "source": [
    "https://www.tensorflow.org/recommenders/examples/deep_recommenders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrDVNe7Vdqhr"
   },
   "source": [
    "In [the featurization tutorial](featurization) we incorporated multiple features into our models, but the models consist of only an embedding layer. We can add more dense layers to our models to increase their expressive power.\n",
    "\n",
    "In general, deeper models are capable of learning more complex patterns than shallower models. For example, our [user model](featurization#user_model) incorporates user ids and timestamps to model user preferences at a point in time. A shallow model (say, a single embedding layer) may only be able to learn the simplest relationships between those features and movies: a given movie is most popular around the time of its release, and a given user generally prefers horror movies to comedies. To capture more complex relationships, such as user preferences evolving over time, we may need a deeper model with multiple stacked dense layers.\n",
    "\n",
    "Of course, complex models also have their disadvantages. The first is computational cost, as larger models require both more memory and more computation to fit and serve. The second is the requirement for more data: in general, more training data is needed to take advantage of deeper models. With more parameters, deep models might overfit or even simply memorize the training examples instead of learning a function that can generalize. Finally, training deeper models may be harder, and more care needs to be taken in choosing settings like regularization and learning rate.\n",
    "\n",
    "Finding a good architecture for a real-world recommender system is a complex art, requiring good intuition and careful [hyperparameter tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization). For example, factors such as the depth and width of the model, activation function, learning rate, and optimizer can radically change the performance of the model. Modelling choices are further complicated by the fact that good offline evaluation metrics may not correspond to good online performance, and that the choice of what to optimize for is often more critical than the choice of model itself.\n",
    "\n",
    "Nevertheless, effort put into building and fine-tuning larger models often pays off. In this tutorial, we will illustrate how to build deep retrieval models using TensorFlow Recommenders. We'll do this by building progressively more complex models to see how this affects model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7RYXwgbAcbU"
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "We first import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T11:13:02.980559Z",
     "iopub.status.busy": "2021-08-14T11:13:02.979894Z",
     "iopub.status.idle": "2021-08-14T11:13:05.639645Z",
     "shell.execute_reply": "2021-08-14T11:13:05.639128Z"
    },
    "id": "XbwMjnLP5nZ_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgKIjpQLAiax"
   },
   "source": [
    "In this tutorial we will use the models from [the featurization tutorial](featurization) to generate embeddings. Hence we will only be using the user id, timestamp, and movie title features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_file_cold(file, stats):\n",
    "    print('loading file:' + file)\n",
    "    training_df = pd.read_csv(\n",
    "        file,\n",
    "        skiprows=[0],\n",
    "        names=[\"viewer\",\"broadcaster\",\"viewer_age\",\"viewer_gender\",\"viewer_longitude\",\"viewer_latitude\",\"viewer_lang\",\"viewer_country\",\"broadcaster_age\",\"broadcaster_gender\",\"broadcaster_longitude\",\"broadcaster_latitude\",\"broadcaster_lang\",\"broadcaster_country\",\"duration\", \"viewer_network\", \"broadcaster_network\", \"count\"], dtype={\n",
    "            'viewer': np.unicode,\n",
    "            'broadcaster': np.unicode,\n",
    "            'viewer_age': np.single,\n",
    "            'viewer_gender': np.unicode,\n",
    "            'viewer_longitude': np.single,\n",
    "            'viewer_latitude': np.single,\n",
    "            'viewer_lang': np.unicode,\n",
    "            'viewer_country': np.unicode,\n",
    "            'broadcaster_age': np.single,\n",
    "            'broadcaster_longitude': np.single,\n",
    "            'broadcaster_latitude': np.single,\n",
    "            'broadcaster_lang': np.unicode,\n",
    "            'broadcaster_country': np.unicode,\n",
    "            'viewer_network': np.unicode,\n",
    "            'broadcaster_network': np.unicode,\n",
    "            'count': np.int\n",
    "        })\n",
    "\n",
    "    values = {\n",
    "        'viewer': 'unknown',\n",
    "        'broadcaster': 'unknown',\n",
    "        'viewer_age': 30,\n",
    "        'viewer_gender': 'unknown',\n",
    "        'viewer_longitude': 0,\n",
    "        'viewer_latitude': 0,\n",
    "        'viewer_lang': 'unknown',\n",
    "        'viewer_country': 'unknown',\n",
    "        'broadcaster_age': 30,\n",
    "        'broadcaster_longitude': 0,\n",
    "        'broadcaster_latitude': 0,\n",
    "        'broadcaster_lang': 'unknown',\n",
    "        'broadcaster_country': 'unknown',\n",
    "        'duration': 0,\n",
    "        'viewer_network': 'unknown',\n",
    "        'broadcaster_network': 'unknown',\n",
    "        'count': 0\n",
    "    }\n",
    "    training_df.fillna(value=values, inplace=True)\n",
    "#     print(training_df.head(10))\n",
    "#     print(training_df.iloc[-10:])\n",
    "#     stats.send_stats('data-size', len(training_df.index))\n",
    "\n",
    "    sampled_df = training_df.sample(frac=0.1)\n",
    "    print(sampled_df.head(10))\n",
    "    print(sampled_df.iloc[-10:])\n",
    "    return sampled_df\n",
    "\n",
    "def load_training_data_cold(file, stats):\n",
    "    ratings_df = load_data_file_cold(file, stats)\n",
    "    print('creating data set')\n",
    "    training_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices(\n",
    "            ({\n",
    "                \"viewer\": tf.cast(\n",
    "                    ratings_df['viewer'].values,\n",
    "                    tf.string),\n",
    "                \"viewer_gender\": tf.cast(\n",
    "                    ratings_df['viewer_gender'].values,\n",
    "                    tf.string),\n",
    "                \"viewer_lang\": tf.cast(\n",
    "                    ratings_df['viewer_lang'].values,\n",
    "                    tf.string),\n",
    "                \"viewer_country\": tf.cast(\n",
    "                    ratings_df['viewer_country'].values,\n",
    "                    tf.string),\n",
    "                \"viewer_age\": tf.cast(\n",
    "                    ratings_df['viewer_age'].values,\n",
    "                    tf.int32),\n",
    "                \"viewer_longitude\": tf.cast(\n",
    "                    ratings_df['viewer_longitude'].values,\n",
    "                    tf.float16),\n",
    "                \"viewer_latitude\": tf.cast(\n",
    "                    ratings_df['viewer_latitude'].values,\n",
    "                    tf.float16),\n",
    "                \"broadcaster\": tf.cast(\n",
    "                    ratings_df['broadcaster'].values,\n",
    "                    tf.string),\n",
    "                \"viewer_network\": tf.cast(\n",
    "                    ratings_df['viewer_network'].values,\n",
    "                    tf.string),\n",
    "                \"broadcaster_network\": tf.cast(\n",
    "                    ratings_df['broadcaster_network'].values,\n",
    "                    tf.string),\n",
    "            })))\n",
    "\n",
    "    return training_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T11:13:05.645133Z",
     "iopub.status.busy": "2021-08-14T11:13:05.644503Z",
     "iopub.status.idle": "2021-08-14T11:13:07.206138Z",
     "shell.execute_reply": "2021-08-14T11:13:07.205653Z"
    },
    "id": "kc2REbOO52Fl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file:a3d86f3b-eb45-4641-b05d-30dff7423e6b.csv\n",
      "                   viewer       broadcaster  viewer_age viewer_gender  \\\n",
      "1938231     pof:333553197     pof:322045884        48.0          male   \n",
      "3207580  meetme:291011164  meetme:312245706        30.0          male   \n",
      "2528752     pof:326740458  meetme:266264990        34.0        female   \n",
      "5037884    skout:69436719   skout:178518892        45.0          male   \n",
      "5224811  meetme:278962360  meetme:275412896        21.0          male   \n",
      "2677221  meetme:316544521     pof:192040040        66.0          male   \n",
      "4143312     pof:333471872     pof:322761842        31.0          male   \n",
      "2444346     pof:307744471  meetme:152006167        39.0          male   \n",
      "5304332  meetme:317948716  meetme:317712363        25.0          male   \n",
      "1868980  meetme:201849202  meetme:314739694        36.0          male   \n",
      "\n",
      "         viewer_longitude  viewer_latitude viewer_lang viewer_country  \\\n",
      "1938231        -88.000000        43.000000          en             US   \n",
      "3207580       -118.209999        33.977699          en             US   \n",
      "2528752          0.500000        51.599998          en             US   \n",
      "5037884        129.231003        35.561001          ko             KR   \n",
      "5224811        -94.372803        44.890202          en             US   \n",
      "2677221        -83.053902        42.642899          en             US   \n",
      "4143312         11.800000        49.200001          en             US   \n",
      "2444346          1.300000        52.599998          en             US   \n",
      "5304332        -76.325996       -12.237400          es             ES   \n",
      "1868980         74.722801        27.017799          en             US   \n",
      "\n",
      "         broadcaster_age broadcaster_gender  broadcaster_longitude  \\\n",
      "1938231             26.0             female             -74.000000   \n",
      "3207580             24.0             female             -97.716698   \n",
      "2528752             30.0               male             -84.242500   \n",
      "5037884             36.0             female             128.938004   \n",
      "5224811             27.0             female             -84.242500   \n",
      "2677221             35.0             female             -96.099998   \n",
      "4143312             28.0             female             -84.400002   \n",
      "2444346             37.0             female               0.110400   \n",
      "5304332             21.0             female             -74.795898   \n",
      "1868980             53.0             female              92.673500   \n",
      "\n",
      "         broadcaster_latitude broadcaster_lang broadcaster_country  duration  \\\n",
      "1938231             40.799999               en                  US        88   \n",
      "3207580             30.455400               en                  US       389   \n",
      "2528752             34.029900               en                  US      1082   \n",
      "5037884             35.960999               ko                  KR        78   \n",
      "5224811             34.029900               en                  US       282   \n",
      "2677221             41.200001               en                  US        62   \n",
      "4143312             33.700001               en                  US       224   \n",
      "2444346             51.696098               en                  GB       425   \n",
      "5304332             10.970300               es                  NI       217   \n",
      "1868980             11.700100               en                  IN       265   \n",
      "\n",
      "        viewer_network broadcaster_network  count  \n",
      "1938231            pof                 pof      1  \n",
      "3207580         meetme              meetme      4  \n",
      "2528752            pof              meetme      1  \n",
      "5037884          skout               skout      1  \n",
      "5224811         meetme              meetme      4  \n",
      "2677221         meetme                 pof      1  \n",
      "4143312            pof                 pof      1  \n",
      "2444346            pof              meetme      2  \n",
      "5304332         meetme              meetme      1  \n",
      "1868980         meetme              meetme      4  \n",
      "                   viewer       broadcaster  viewer_age viewer_gender  \\\n",
      "3704225  meetme:217857296  meetme:317917078        25.0        female   \n",
      "3327286  meetme:270514230  meetme:255559696        24.0        female   \n",
      "4840084   skout:158870706   skout:164069692        28.0          male   \n",
      "857316   meetme:267909178  meetme:309387539        28.0          male   \n",
      "760556    skout:115099344   skout:164637512        39.0          male   \n",
      "2546408  meetme:247890289  meetme:308858268        24.0        female   \n",
      "2084981   skout:183786557     pof:187778808        21.0        female   \n",
      "3562507     pof:131720942     pof:331739554        51.0          male   \n",
      "3108     meetme:187320843   skout:139683230        22.0          male   \n",
      "1188049  meetme:233652161  meetme:243943889        38.0          male   \n",
      "\n",
      "         viewer_longitude  viewer_latitude viewer_lang viewer_country  \\\n",
      "3704225        -73.830002        40.830002          en             US   \n",
      "3327286       -118.239304        34.063400          en             US   \n",
      "4840084         50.209999        26.288000          en             GB   \n",
      "857316          77.102501        28.704100          en             US   \n",
      "760556         174.623001       -36.860001          en             NZ   \n",
      "2546408        -98.605301        29.572100          en             US   \n",
      "2084981        -76.893997        40.278000          en             US   \n",
      "3562507        147.100006       -42.799999          en             US   \n",
      "3108            77.171303        28.660101          en             US   \n",
      "1188049        -98.411499        29.679199          en             US   \n",
      "\n",
      "         broadcaster_age broadcaster_gender  broadcaster_longitude  \\\n",
      "3704225             29.0               male             -73.830002   \n",
      "3327286             35.0               male            -118.294197   \n",
      "4840084             39.0             female              46.785999   \n",
      "857316              28.0               male              72.709702   \n",
      "760556              28.0             female             121.037003   \n",
      "2546408             23.0               male             -98.527603   \n",
      "2084981             38.0               male             -75.699997   \n",
      "3562507             34.0             female             145.300003   \n",
      "3108                27.0             female              77.257004   \n",
      "1188049             28.0             female             -98.695702   \n",
      "\n",
      "         broadcaster_latitude broadcaster_lang broadcaster_country  duration  \\\n",
      "3704225             40.830002               en                  US       382   \n",
      "3327286             34.048199               en                  US       169   \n",
      "4840084             24.846001               en                  SA        92   \n",
      "857316              31.170401               en                  PK       722   \n",
      "760556              14.512000               en                  PH        63   \n",
      "2546408             29.420900               en                  US        66   \n",
      "2084981             40.000000               en                  US       160   \n",
      "3562507            -38.000000               en                  AU       145   \n",
      "3108                28.544001               en                  IN        81   \n",
      "1188049             29.326200               en                  US       183   \n",
      "\n",
      "        viewer_network broadcaster_network  count  \n",
      "3704225         meetme              meetme      1  \n",
      "3327286         meetme              meetme      1  \n",
      "4840084          skout               skout      1  \n",
      "857316          meetme              meetme      1  \n",
      "760556           skout               skout      1  \n",
      "2546408         meetme              meetme      1  \n",
      "2084981          skout                 pof      2  \n",
      "3562507            pof                 pof      2  \n",
      "3108            meetme               skout      1  \n",
      "1188049         meetme              meetme      3  \n",
      "creating data set\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fe7de9bd8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7fe7de9bd8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fe7de9bdf80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7fe7de9bdf80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "ratings = load_training_data_cold(file=\"a3d86f3b-eb45-4641-b05d-30dff7423e6b.csv\", stats=\"\")\n",
    "\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"broadcaster\": x[\"broadcaster\"],\n",
    "    \"viewer\": x[\"viewer\"],   \n",
    "    \"viewer_age\": x[\"viewer_age\"],\n",
    "})\n",
    "\n",
    "broadcaster = ratings.map(lambda x: x[\"broadcaster\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YZ2q5RXYNI6"
   },
   "source": [
    "We also do some housekeeping to prepare feature vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T11:13:07.212602Z",
     "iopub.status.busy": "2021-08-14T11:13:07.212043Z",
     "iopub.status.idle": "2021-08-14T11:13:11.211481Z",
     "shell.execute_reply": "2021-08-14T11:13:11.210956Z"
    },
    "id": "G5CVveCS9Doq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fe7fae6d4d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7fe7fae6d4d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fe7fae6ddd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7fe7fae6ddd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# Discretization\n",
    "max_viewer_age = ratings.map(lambda x: x[\"viewer_age\"]).reduce(\n",
    "    tf.cast(0, tf.int32), tf.maximum).numpy().max()\n",
    "min_viewer_age = ratings.map(lambda x: x[\"viewer_age\"]).reduce(\n",
    "    np.int32(100), tf.minimum).numpy().min()\n",
    "\n",
    "viewer_age_buckets = np.linspace(\n",
    "    min_viewer_age, max_viewer_age, num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fe7fae6d560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7fe7fae6d560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "249546"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ids = ratings.batch(1_00_000).map(lambda x: x[\"viewer\"])\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "len(unique_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fe78e21f830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7fe78e21f830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69801"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcaster_ids = ratings.batch(1_00_000).map(lambda x: x[\"broadcaster\"])\n",
    "unique_broadcaster_ids = np.unique(np.concatenate(list(broadcaster_ids)))\n",
    "len(unique_broadcaster_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFJcCVMUQou3"
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtS6a4sgmI-c"
   },
   "source": [
    "### Query model - UserModel\n",
    "\n",
    "We start with the user model defined in [the featurization tutorial](featurization) as the first layer of our model, tasked with converting raw input examples into feature embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fe78e53bef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7fe78e53bef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "viewer_ages = np.concatenate(list(ratings.map(lambda x: x[\"viewer_age\"]).batch(1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T11:13:11.219142Z",
     "iopub.status.busy": "2021-08-14T11:13:11.214393Z",
     "iopub.status.idle": "2021-08-14T11:13:11.220882Z",
     "shell.execute_reply": "2021-08-14T11:13:11.221245Z"
    },
    "id": "_ItzYwMW42cb"
   },
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                vocabulary=unique_user_ids, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
    "        ])\n",
    "        \n",
    "        self.viewer_age_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.Discretization(viewer_age_buckets.tolist()),\n",
    "            tf.keras.layers.Embedding(len(viewer_age_buckets) + 1, 32),\n",
    "        ])\n",
    "        \n",
    "        self.normalized_viewer_age = tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "            axis=None\n",
    "        )\n",
    "\n",
    "        self.normalized_viewer_age.adapt(viewer_ages)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Take the input dictionary, pass it through each input layer,\n",
    "        # and concatenate the result.\n",
    "        return tf.concat([\n",
    "            self.user_embedding(inputs[\"viewer\"]),\n",
    "            self.viewer_age_embedding(inputs[\"viewer_age\"]),\n",
    "            tf.reshape(self.normalized_viewer_age(inputs[\"viewer_age\"]), (-1, 1)),\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMQzxLqh42on"
   },
   "source": [
    "Defining deeper models will require us to stack mode layers on top of this first input. A progressively narrower stack of layers, separated by an activation function, is a common pattern:\n",
    "\n",
    "```\n",
    "                            +----------------------+\n",
    "                            |      128 x 64        |\n",
    "                            +----------------------+\n",
    "                                       | relu\n",
    "                          +--------------------------+\n",
    "                          |        256 x 128         |\n",
    "                          +--------------------------+\n",
    "                                       | relu\n",
    "                        +------------------------------+\n",
    "                        |          ... x 256           |\n",
    "                        +------------------------------+\n",
    "```\n",
    "Since the expressive power of deep linear models is no greater than that of shallow linear models, we use ReLU activations for all but the last hidden layer. The final hidden layer does not use any activation function: using an activation function would limit the output space of the final embeddings and might negatively impact the performance of the model. For instance, if ReLUs are used in the projection layer, all components in the output embedding would be non-negative.\n",
    "\n",
    "We're going to try something similar here. To make experimentation with different depths easy, let's define a model whose depth (and width) is defined by a set of constructor parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T11:13:11.227987Z",
     "iopub.status.busy": "2021-08-14T11:13:11.227358Z",
     "iopub.status.idle": "2021-08-14T11:13:11.229637Z",
     "shell.execute_reply": "2021-08-14T11:13:11.229235Z"
    },
    "id": "5qfPi4I-Z0ph"
   },
   "outputs": [],
   "source": [
    "class QueryModel ( tf.keras.Model ):\n",
    "\t\"\"\"Model for encoding user queries.\"\"\"\n",
    "\n",
    "\tdef __init__ ( self , layer_sizes ):\n",
    "\t\t\"\"\"Model for encoding user queries.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t  layer_sizes:\n",
    "\t\t\tA list of integers where the i-th entry represents the number of units\n",
    "\t\t\tthe i-th layer contains.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper ( ).__init__ ( )\n",
    "\n",
    "\t\t# We first use the user model for generating embeddings.\n",
    "\t\tself.embedding_model = UserModel ( )\n",
    "\n",
    "\t\t# Then construct the layers.\n",
    "\t\tself.dense_layers = tf.keras.Sequential ( )\n",
    "\n",
    "\t\t# Use the ReLU activation for all but the last layer.\n",
    "\t\tfor layer_size in layer_sizes [ :-1 ]:\n",
    "\t\t\tself.dense_layers.add ( tf.keras.layers.Dense ( layer_size , activation = \"relu\" ) )\n",
    "\n",
    "\t\t# No activation for the last layer.\n",
    "\t\tfor layer_size in layer_sizes [ -1: ]:\n",
    "\t\t\tself.dense_layers.add ( tf.keras.layers.Dense ( layer_size ) )\n",
    "\n",
    "\tdef call ( self , inputs ):\n",
    "\t\tfeature_embedding = self.embedding_model ( inputs )\n",
    "\t\treturn self.dense_layers ( feature_embedding )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9IqNTLmpJzs"
   },
   "source": [
    "The `layer_sizes` parameter gives us the depth and width of the model. We can vary it to experiment with shallower or deeper models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XleMceZNHC__"
   },
   "source": [
    "### Candidate model - BroadcasterModel\n",
    "\n",
    "We can adopt the same approach for the movie model. Again, we start with the `MovieModel` from the [featurization](featurization) tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_on_colons(text):\n",
    "    return tf.strings.split(text, sep=\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T11:13:11.235958Z",
     "iopub.status.busy": "2021-08-14T11:13:11.235350Z",
     "iopub.status.idle": "2021-08-14T11:13:11.236990Z",
     "shell.execute_reply": "2021-08-14T11:13:11.237295Z"
    },
    "id": "oQZHX8bEHPOk"
   },
   "outputs": [],
   "source": [
    "class BroadcasterModel(tf.keras.Model):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tembedding_dimension = 32\n",
    "\t\tmax_tokens = 32\n",
    "\n",
    "\t\tself.broadcaster_embedding = tf.keras.Sequential([\n",
    "\t\t\ttf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "\t\t\t\tvocabulary=unique_broadcaster_ids, mask_token=None),\n",
    "\t\t\ttf.keras.layers.Embedding(len(unique_broadcaster_ids) + 1, embedding_dimension)\n",
    "\t\t])\n",
    "\n",
    "\t\tself.broadcaster_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "\t\t\tstandardize= None, split=split_on_colons, max_tokens = max_tokens)\n",
    "\n",
    "\t\tself.broadcaster_text_embedding = tf.keras.Sequential([\n",
    "\t\t\tself.broadcaster_vectorizer,\n",
    "\t\t\ttf.keras.layers.Embedding(max_tokens, embedding_dimension, mask_zero=True),\n",
    "\t\t\ttf.keras.layers.GlobalAveragePooling1D(),\n",
    "\t\t])\n",
    "\n",
    "\t\tself.broadcaster_vectorizer.adapt(broadcaster)\n",
    "\n",
    "\tdef call(self, broadcaster):\n",
    "\t\treturn tf.concat([\n",
    "\t\t\tself.broadcaster_embedding(broadcaster),\n",
    "\t\t\tself.broadcaster_text_embedding(broadcaster),\n",
    "\t\t], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6vssqPYp-gY"
   },
   "source": [
    "And expand it with hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T11:13:11.242812Z",
     "iopub.status.busy": "2021-08-14T11:13:11.242131Z",
     "iopub.status.idle": "2021-08-14T11:13:11.245094Z",
     "shell.execute_reply": "2021-08-14T11:13:11.244535Z"
    },
    "id": "l1gTXkvQqHGA"
   },
   "outputs": [],
   "source": [
    "class CandidateModel(tf.keras.Model):\n",
    "\t\"\"\"Model for encoding movies.\"\"\"\n",
    "\n",
    "\tdef __init__(self, layer_sizes):\n",
    "\t\t\"\"\"Model for encoding movies.\n",
    "\t\n",
    "\t\tArgs:\n",
    "\t\t  layer_sizes:\n",
    "\t\t\tA list of integers where the i-th entry represents the number of units\n",
    "\t\t\tthe i-th layer contains.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.embedding_model = BroadcasterModel()\n",
    "\n",
    "\t\t# Then construct the layers.\n",
    "\t\tself.dense_layers = tf.keras.Sequential()\n",
    "\n",
    "\t\t# Use the ReLU activation for all but the last layer.\n",
    "\t\tfor layer_size in layer_sizes[:-1]:\n",
    "\t\t\tself.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "\n",
    "\t\t# No activation for the last layer.\n",
    "\t\tfor layer_size in layer_sizes[-1:]:\n",
    "\t\t\tself.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
    "\n",
    "\tdef call(self, inputs):\n",
    "\t\tfeature_embedding = self.embedding_model(inputs)\n",
    "\t\treturn self.dense_layers(feature_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cc4KbTNwHSvD"
   },
   "source": [
    "### Combined model\n",
    "\n",
    "With both `QueryModel` and `CandidateModel` defined, we can put together a combined model and implement our loss and metrics logic. To make things simple, we'll enforce that the model structure is the same across the query and candidate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T11:13:11.251189Z",
     "iopub.status.busy": "2021-08-14T11:13:11.250595Z",
     "iopub.status.idle": "2021-08-14T11:13:11.252789Z",
     "shell.execute_reply": "2021-08-14T11:13:11.252412Z"
    },
    "id": "26_hNJPKIh4-"
   },
   "outputs": [],
   "source": [
    "class FinalModel ( tfrs.models.Model ):\n",
    "\n",
    "\tdef __init__ ( self , layer_sizes ):\n",
    "\t\tsuper ( ).__init__ ( )\n",
    "\t\tself.query_model = QueryModel ( layer_sizes )\n",
    "\t\tself.candidate_model = CandidateModel ( layer_sizes )\n",
    "\t\tself.task = tfrs.tasks.Retrieval (\n",
    "\t\t\tmetrics = tfrs.metrics.FactorizedTopK (\n",
    "\t\t\t\tcandidates = broadcasters.batch(256).map (self.candidate_model) ,\n",
    "\t\t\t) ,\n",
    "\t\t)\n",
    "\n",
    "\tdef compute_loss ( self , features , training = False ):\n",
    "\t\t# We only pass the user id and timestamp features into the query model. This\n",
    "\t\t# is to ensure that the training inputs would have the same keys as the\n",
    "\t\t# query inputs. Otherwise the discrepancy in input structure would cause an\n",
    "\t\t# error when loading the query model after saving it.\n",
    "\t\tquery_embeddings = self.query_model ( {\n",
    "\t\t\t\"viewer\": features [ \"viewer\" ] ,\n",
    "\t\t\t\"viewer_age\": features [ \"viewer_age\" ] ,\n",
    "\t\t} )\n",
    "\t\tbroadcaster_embeddings = self.candidate_model ( features [ \"broadcaster\" ] )\n",
    "\n",
    "\t\treturn self.task (\n",
    "\t\t\tquery_embeddings , broadcaster_embeddings , compute_metrics = not training )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YXjsRsLTVzt"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QY7MTwMruoKh"
   },
   "source": [
    "### Prepare the data\n",
    "\n",
    "We first split the data into a training set and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T11:13:11.257899Z",
     "iopub.status.busy": "2021-08-14T11:13:11.257307Z",
     "iopub.status.idle": "2021-08-14T11:13:11.263191Z",
     "shell.execute_reply": "2021-08-14T11:13:11.262737Z"
    },
    "id": "wMFUZ4dyTdYd"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(16384)\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2HEuTBzJ9w5"
   },
   "source": [
    "### Shallow model\n",
    "\n",
    "We're ready to try out our first, shallow, model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T11:13:11.268304Z",
     "iopub.status.busy": "2021-08-14T11:13:11.267713Z",
     "iopub.status.idle": "2021-08-14T11:24:58.236270Z",
     "shell.execute_reply": "2021-08-14T11:24:58.235774Z"
    },
    "id": "NkoLkiQdK4Um"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fe84ea9b950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fe84ea9b950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fe78b23b5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fe78b23b5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method CandidateModel.call of <__main__.CandidateModel object at 0x7fe7a118c7d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method CandidateModel.call of <__main__.CandidateModel object at 0x7fe7a118c7d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method BroadcasterModel.call of <__main__.BroadcasterModel object at 0x7fe7a10f99d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method BroadcasterModel.call of <__main__.BroadcasterModel object at 0x7fe7a10f99d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe7a5b6ff80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe7a5b6ff80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method QueryModel.call of <__main__.QueryModel object at 0x7fe7a1162210>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method QueryModel.call of <__main__.QueryModel object at 0x7fe7a1162210>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method UserModel.call of <__main__.UserModel object at 0x7fe7a1168a10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method UserModel.call of <__main__.UserModel object at 0x7fe7a1168a10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Retrieval.call of <tensorflow_recommenders.tasks.retrieval.Retrieval object at 0x7fe7a10ded50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Retrieval.call of <tensorflow_recommenders.tasks.retrieval.Retrieval object at 0x7fe7a10ded50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_factorized_top_k/top_100_categorical_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-383cc3bcc0ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     verbose=0)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_layer_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_factorized_top_k/top_100_categorical_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Top-100 accuracy: {accuracy:.2f}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_factorized_top_k/top_100_categorical_accuracy'"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "model = FinalModel([32])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "one_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=5,\n",
    "    epochs=num_epochs,\n",
    "    verbose=0)\n",
    "\n",
    "accuracy = one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "print(f\"Top-100 accuracy: {accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p90vFk8LvJXp"
   },
   "source": [
    "This gives us a top-100 accuracy of around 0.27. We can use this as a reference point for evaluating deeper models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjJ1anzuLXgN"
   },
   "source": [
    "### Deeper model\n",
    "\n",
    "What about a deeper model with two layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T11:24:58.243309Z",
     "iopub.status.busy": "2021-08-14T11:24:58.242682Z",
     "iopub.status.idle": "2021-08-14T11:36:47.037515Z",
     "shell.execute_reply": "2021-08-14T11:36:47.037044Z"
    },
    "id": "11qAr5gGMUxE"
   },
   "outputs": [],
   "source": [
    "model = FinalModel([64, 32])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "two_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=5,\n",
    "    epochs=num_epochs,\n",
    "    verbose=0)\n",
    "\n",
    "accuracy = two_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "print(f\"Top-100 accuracy: {accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHnzYfQrOj8I"
   },
   "source": [
    "The accuracy here is 0.29, quite a bit better than the shallow model.\n",
    "\n",
    "We can plot the validation accuracy curves to illustrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T11:36:47.056175Z",
     "iopub.status.busy": "2021-08-14T11:36:47.047659Z",
     "iopub.status.idle": "2021-08-14T11:36:47.284984Z",
     "shell.execute_reply": "2021-08-14T11:36:47.285367Z"
    },
    "id": "xzriiDRlHEvo"
   },
   "outputs": [],
   "source": [
    "num_validation_runs = len(one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"])\n",
    "epochs = [(x + 1)* 5 for x in range(num_validation_runs)]\n",
    "\n",
    "plt.plot(epochs, one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"1 layer\")\n",
    "plt.plot(epochs, two_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"2 layers\")\n",
    "plt.title(\"Accuracy vs epoch\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Top-100 accuracy\");\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ItwGCpXj9YF"
   },
   "source": [
    "Even early on in the training, the larger model has a clear and stable lead over the shallow model, suggesting that adding depth helps the model capture more nuanced relationships in the data.\n",
    "\n",
    "However, even deeper models are not necessarily better. The following model extends the depth to three layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T11:36:47.292771Z",
     "iopub.status.busy": "2021-08-14T11:36:47.292143Z",
     "iopub.status.idle": "2021-08-14T11:48:44.368987Z",
     "shell.execute_reply": "2021-08-14T11:48:44.369389Z"
    },
    "id": "es9k4o0ROt0l"
   },
   "outputs": [],
   "source": [
    "model = MovielensModel([128, 64, 32])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "three_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=5,\n",
    "    epochs=num_epochs,\n",
    "    verbose=0)\n",
    "\n",
    "accuracy = three_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "print(f\"Top-100 accuracy: {accuracy:.2f}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLJV8jut40Ur"
   },
   "source": [
    "In fact, we don't see improvement over the shallow model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T11:48:44.387028Z",
     "iopub.status.busy": "2021-08-14T11:48:44.386335Z",
     "iopub.status.idle": "2021-08-14T11:48:44.549113Z",
     "shell.execute_reply": "2021-08-14T11:48:44.549484Z"
    },
    "id": "pIoVoMO1Kav6"
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"1 layer\")\n",
    "plt.plot(epochs, two_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"2 layers\")\n",
    "plt.plot(epochs, three_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"3 layers\")\n",
    "plt.title(\"Accuracy vs epoch\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Top-100 accuracy\");\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wC95C1anA5Gx"
   },
   "source": [
    "This is a good illustration of the fact that deeper and larger models, while capable of superior performance, often require very careful tuning. For example, throughout this tutorial we used a single, fixed learning rate. Alternative choices may give very different results and are worth exploring. \n",
    "\n",
    "With appropriate tuning and sufficient data, the effort put into building larger and deeper models is in many cases well worth it: larger models can lead to substantial improvements in prediction accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dB09crfpgBx7"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "In this tutorial we expanded our retrieval model with dense layers and activation functions. To see how to create a model that can perform not only retrieval tasks but also rating tasks, take a look at [the multitask tutorial](multitask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "deep_recommenders.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
