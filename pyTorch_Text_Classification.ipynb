{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyTorch_Text_Classification.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linghduoduo/Deep-Learning/blob/master/pyTorch_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCoJtBrWnGdr",
        "outputId": "bd8ad314-e411-46b6-e06b-c68622ecc53e"
      },
      "source": [
        "!pip3 install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 55.9MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 53.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=4070598d0bac0067a7eb9ea7dbc974437d7a958874d7592ff4a9bbb3077fe42d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez2sMGnDn1Pt",
        "outputId": "d022aee2-a588-444b-a998-f9609cc9f159"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Feb 16 23:06:29 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0ej2wVtpd9F"
      },
      "source": [
        "from collections import defaultdict\n",
        "import time\n",
        "from joblib import Parallel, delayed\n",
        "from multiprocessing import cpu_count\n",
        "from math import ceil"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSLPNWM3ppHe"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.multiprocessing as mp\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import RandomSampler"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rBbZ3h0pvmM"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lrEsFdwp4B4"
      },
      "source": [
        "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s01YoYoXqAhF"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZeejXZ8qAit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca58829c-b1c3-4f3a-a19a-be4225d9609b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "workspace_dir = './drive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vExuE9tJqdG9"
      },
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "args = Namespace(\n",
        "    max_len=200,\n",
        "    dataset_dir=workspace_dir,\n",
        "    label_names_file=\"data/agnews/label_names.txt\",\n",
        "    train_file=\"data/agnews/train.txt\",\n",
        "    test_file=\"data/agnews/test.txt\",\n",
        "    test_label_file=\"data/agnews/test_labels.txt\",\n",
        "    train_batch_size=32,\n",
        "    accum_steps=4,\n",
        "    eval_batch_size=128,\n",
        "    gpus=1,\n",
        "    mcp_epochs=3,\n",
        "    self_train_epochs=1,\n",
        "    dist_port=12345,\n",
        "    update_interval=50,\n",
        "    early_stop=True,\n",
        "    top_pred_num=50,\n",
        "    category_vocab_size=100\n",
        ")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmlfHb_gzEOu"
      },
      "source": [
        "from transformers import BertPreTrainedModel, BertModel\n",
        "from transformers.models.bert.modeling_bert import BertOnlyMLMHead\n",
        "from torch import nn\n",
        "import sys"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M93SToy4y-xM"
      },
      "source": [
        "class LOTClassModel(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        self.cls = BertOnlyMLMHead(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        self.init_weights()\n",
        "        # MLM head is not trained\n",
        "        for param in self.cls.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    def forward(self, input_ids, pred_mode, attention_mask=None, token_type_ids=None, \n",
        "                position_ids=None, head_mask=None, inputs_embeds=None):\n",
        "        bert_outputs = self.bert(input_ids,\n",
        "                                 attention_mask=attention_mask,\n",
        "                                 token_type_ids=token_type_ids,\n",
        "                                 position_ids=position_ids,\n",
        "                                 head_mask=head_mask,\n",
        "                                 inputs_embeds=inputs_embeds)\n",
        "        last_hidden_states = bert_outputs[0]\n",
        "        if pred_mode == \"classification\":\n",
        "            trans_states = self.dense(last_hidden_states)\n",
        "            trans_states = self.activation(trans_states)\n",
        "            trans_states = self.dropout(trans_states)\n",
        "            logits = self.classifier(trans_states)\n",
        "        elif pred_mode == \"mlm\":\n",
        "            logits = self.cls(last_hidden_states)\n",
        "        else:\n",
        "            sys.exit(\"Wrong pred_mode!\")\n",
        "        return logits"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ6gNgTbwiZc"
      },
      "source": [
        "class LOTClassTrainer(object):\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.max_len = args.max_len\n",
        "        self.dataset_dir = args.dataset_dir\n",
        "        self.dist_port = args.dist_port\n",
        "        self.num_cpus = min(10, cpu_count() - 1) if cpu_count() > 1 else 1\n",
        "        self.world_size = args.gpus\n",
        "        self.train_batch_size = args.train_batch_size\n",
        "        self.eval_batch_size = args.eval_batch_size\n",
        "        self.accum_steps = args.accum_steps\n",
        "        eff_batch_size = self.train_batch_size * self.world_size * self.accum_steps\n",
        "        assert abs(eff_batch_size - 128) < 10, f\"Make sure the effective training batch size is around 128, current: {eff_batch_size}\"\n",
        "        print(f\"Effective training batch size: {eff_batch_size}\")\n",
        "        self.pretrained_lm = 'bert-base-uncased'\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.pretrained_lm, do_lower_case=True)\n",
        "        self.vocab = self.tokenizer.get_vocab()\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.mask_id = self.vocab[self.tokenizer.mask_token]\n",
        "        self.inv_vocab = {k:v for v, k in self.vocab.items()}\n",
        "        self.read_label_names(args.dataset_dir, args.label_names_file)\n",
        "        self.num_class = len(self.label_name_dict)\n",
        "        self.model = LOTClassModel.from_pretrained(self.pretrained_lm,\n",
        "                                                   output_attentions=False,\n",
        "                                                   output_hidden_states=False,\n",
        "                                                   num_labels=self.num_class)\n",
        "        self.read_data(args.dataset_dir, args.train_file, args.test_file, args.test_label_file)\n",
        "        self.with_test_label = True if args.test_label_file is not None else False\n",
        "        self.temp_dir = f'tmp_{self.dist_port}'\n",
        "        self.mcp_loss = nn.CrossEntropyLoss()\n",
        "        self.st_loss = nn.KLDivLoss(reduction='batchmean')\n",
        "        self.update_interval = args.update_interval\n",
        "        self.early_stop = args.early_stop\n",
        "    \n",
        "    # read label names from file\n",
        "    def read_label_names(self, dataset_dir, label_name_file):\n",
        "        label_name_file = open(os.path.join(dataset_dir, label_name_file))\n",
        "        label_names = label_name_file.readlines()\n",
        "        self.label_name_dict = {i: [word.lower() for word in category_words.strip().split()] for i, category_words in enumerate(label_names)}\n",
        "        print(f\"Label names used for each class are: {self.label_name_dict}\")\n",
        "        self.label2class = {}\n",
        "        self.all_label_name_ids = [self.mask_id]\n",
        "        self.all_label_names = [self.tokenizer.mask_token]\n",
        "        for class_idx in self.label_name_dict:\n",
        "            for word in self.label_name_dict[class_idx]:\n",
        "                assert word not in self.label2class, f\"\\\"{word}\\\" used as the label name by multiple classes!\"\n",
        "                self.label2class[word] = class_idx\n",
        "                if word in self.vocab:\n",
        "                    self.all_label_name_ids.append(self.vocab[word])\n",
        "                    self.all_label_names.append(word)\n",
        "\n",
        "    # read text corpus and labels from files\n",
        "    def read_data(self, dataset_dir, train_file, test_file, test_label_file):\n",
        "        self.train_data, self.label_name_data = self.create_dataset(dataset_dir, train_file, None, \"train.pt\", \n",
        "                                                                    find_label_name=True, label_name_loader_name=\"label_name_data.pt\")\n",
        "        if test_file is not None:\n",
        "            self.test_data = self.create_dataset(dataset_dir, test_file, test_label_file, \"test.pt\")\n",
        "\n",
        "    # convert dataset into tensors\n",
        "    def create_dataset(self, dataset_dir, text_file, label_file, loader_name, find_label_name=False, label_name_loader_name=None):\n",
        "        loader_file = os.path.join(dataset_dir, loader_name)\n",
        "        if os.path.exists(loader_file):\n",
        "            print(f\"Loading encoded texts from {loader_file}\")\n",
        "            data = torch.load(loader_file)\n",
        "        else:\n",
        "            print(f\"Reading texts from {os.path.join(dataset_dir, text_file)}\")\n",
        "            corpus = open(os.path.join(dataset_dir, text_file), encoding=\"utf-8\")\n",
        "            docs = [doc.strip() for doc in corpus.readlines()]\n",
        "            print(f\"Converting texts into tensors.\")\n",
        "            chunk_size = ceil(len(docs) / self.num_cpus)\n",
        "            chunks = [docs[x:x+chunk_size] for x in range(0, len(docs), chunk_size)]\n",
        "            results = Parallel(n_jobs=self.num_cpus)(delayed(self.encode)(docs=chunk) for chunk in chunks)\n",
        "            input_ids = torch.cat([result[0] for result in results])\n",
        "            attention_masks = torch.cat([result[1] for result in results])\n",
        "            print(f\"Saving encoded texts into {loader_file}\")\n",
        "            if label_file is not None:\n",
        "                print(f\"Reading labels from {os.path.join(dataset_dir, label_file)}\")\n",
        "                truth = open(os.path.join(dataset_dir, label_file))\n",
        "                labels = [int(label.strip()) for label in truth.readlines()]\n",
        "                labels = torch.tensor(labels)\n",
        "                data = {\"input_ids\": input_ids, \"attention_masks\": attention_masks, \"labels\": labels}\n",
        "            else:\n",
        "                data = {\"input_ids\": input_ids, \"attention_masks\": attention_masks}\n",
        "            torch.save(data, loader_file)\n",
        "        if find_label_name:\n",
        "            loader_file = os.path.join(dataset_dir, label_name_loader_name)\n",
        "            if os.path.exists(loader_file):\n",
        "                print(f\"Loading texts with label names from {loader_file}\")\n",
        "                label_name_data = torch.load(loader_file)\n",
        "            else:\n",
        "                print(f\"Reading texts from {os.path.join(dataset_dir, text_file)}\")\n",
        "                corpus = open(os.path.join(dataset_dir, text_file), encoding=\"utf-8\")\n",
        "                docs = [doc.strip() for doc in corpus.readlines()]\n",
        "                print(\"Locating label names in the corpus.\")\n",
        "                chunk_size = ceil(len(docs) / self.num_cpus)\n",
        "                chunks = [docs[x:x+chunk_size] for x in range(0, len(docs), chunk_size)]\n",
        "                results = Parallel(n_jobs=self.num_cpus)(delayed(self.label_name_occurrence)(docs=chunk) for chunk in chunks)\n",
        "                input_ids_with_label_name = torch.cat([result[0] for result in results])\n",
        "                attention_masks_with_label_name = torch.cat([result[1] for result in results])\n",
        "                label_name_idx = torch.cat([result[2] for result in results])\n",
        "                assert len(input_ids_with_label_name) > 0, \"No label names appear in corpus!\"\n",
        "                label_name_data = {\"input_ids\": input_ids_with_label_name, \"attention_masks\": attention_masks_with_label_name, \"labels\": label_name_idx}\n",
        "                loader_file = os.path.join(dataset_dir, label_name_loader_name)\n",
        "                print(f\"Saving texts with label names into {loader_file}\")\n",
        "                torch.save(label_name_data, loader_file)\n",
        "            return data, label_name_data\n",
        "        else:\n",
        "            return data\n",
        "\n",
        "    # convert a list of strings to token ids\n",
        "    def encode(self, docs):\n",
        "        encoded_dict = self.tokenizer.batch_encode_plus(docs, add_special_tokens=True, max_length=self.max_len, padding='max_length',\n",
        "                                                        return_attention_mask=True, truncation=True, return_tensors='pt')\n",
        "        input_ids = encoded_dict['input_ids']\n",
        "        attention_masks = encoded_dict['attention_mask']\n",
        "        return input_ids, attention_masks\n",
        "\n",
        "    # convert list of token ids to list of strings\n",
        "    def decode(self, ids):\n",
        "        strings = self.tokenizer.batch_decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "        return strings\n",
        "    \n",
        "    # find label name occurrences in the corpus\n",
        "    def label_name_occurrence(self, docs):\n",
        "        text_with_label = []\n",
        "        label_name_idx = []\n",
        "        for doc in docs:\n",
        "            result = self.label_name_in_doc(doc)\n",
        "            if result is not None:\n",
        "                text_with_label.append(result[0])\n",
        "                label_name_idx.append(result[1].unsqueeze(0))\n",
        "        if len(text_with_label) > 0:\n",
        "            encoded_dict = self.tokenizer.batch_encode_plus(text_with_label, add_special_tokens=True, max_length=self.max_len, \n",
        "                                                            padding='max_length', return_attention_mask=True, truncation=True, return_tensors='pt')\n",
        "            input_ids_with_label_name = encoded_dict['input_ids']\n",
        "            attention_masks_with_label_name = encoded_dict['attention_mask']\n",
        "            label_name_idx = torch.cat(label_name_idx, dim=0)\n",
        "        else:\n",
        "            input_ids_with_label_name = torch.ones(0, self.max_len, dtype=torch.long)\n",
        "            attention_masks_with_label_name = torch.ones(0, self.max_len, dtype=torch.long)\n",
        "            label_name_idx = torch.ones(0, self.max_len, dtype=torch.long)\n",
        "        return input_ids_with_label_name, attention_masks_with_label_name, label_name_idx\n",
        "      \n",
        "    # find label name indices and replace out-of-vocab label names with [MASK]\n",
        "    def label_name_in_doc(self, doc):\n",
        "        doc = self.tokenizer.tokenize(doc)\n",
        "        label_idx = -1 * torch.ones(self.max_len, dtype=torch.long)\n",
        "        new_doc = []\n",
        "        wordpcs = []\n",
        "        idx = 1 # index starts at 1 due to [CLS] token\n",
        "        for i, wordpc in enumerate(doc):\n",
        "            wordpcs.append(wordpc[2:] if wordpc.startswith(\"##\") else wordpc)\n",
        "            if idx >= self.max_len - 1: # last index will be [SEP] token\n",
        "                break\n",
        "            if i == len(doc) - 1 or not doc[i+1].startswith(\"##\"):\n",
        "                word = ''.join(wordpcs)\n",
        "                if word in self.label2class:\n",
        "                    label_idx[idx] = self.label2class[word]\n",
        "                    # replace label names that are not in tokenizer's vocabulary with the [MASK] token\n",
        "                    if word not in self.vocab:\n",
        "                        wordpcs = [self.tokenizer.mask_token]\n",
        "                new_word = ''.join(wordpcs)\n",
        "                if new_word != self.tokenizer.unk_token:\n",
        "                    idx += len(wordpcs)\n",
        "                    new_doc.append(new_word)\n",
        "                wordpcs = []\n",
        "        if (label_idx >= 0).any():\n",
        "            return ' '.join(new_doc), label_idx\n",
        "        else:\n",
        "            return None\n",
        "      \n",
        "    # construct category vocabulary\n",
        "    def category_vocabulary(self, top_pred_num=50, category_vocab_size=100, loader_name=\"category_vocab.pt\"):\n",
        "        loader_file = os.path.join(self.dataset_dir, loader_name)\n",
        "        if os.path.exists(loader_file):\n",
        "            print(f\"Loading category vocabulary from {loader_file}\")\n",
        "            self.category_vocab = torch.load(loader_file)\n",
        "        else:\n",
        "            print(\"Contructing category vocabulary.\")\n",
        "            if not os.path.exists(self.temp_dir):\n",
        "                os.makedirs(self.temp_dir)\n",
        "            mp.spawn(self.category_vocabulary_dist, nprocs=self.world_size, args=(top_pred_num, loader_name))\n",
        "            gather_res = []\n",
        "            for f in os.listdir(self.temp_dir):\n",
        "                if f[-3:] == '.pt':\n",
        "                    gather_res.append(torch.load(os.path.join(self.temp_dir, f)))\n",
        "            assert len(gather_res) == self.world_size, \"Number of saved files not equal to number of processes!\"\n",
        "            self.category_words_freq = {i: defaultdict(float) for i in range(self.num_class)}\n",
        "            for i in range(self.num_class):\n",
        "                for category_words_freq in gather_res:\n",
        "                    for word_id, freq in category_words_freq[i].items():\n",
        "                        self.category_words_freq[i][word_id] += freq\n",
        "            self.filter_keywords(category_vocab_size)\n",
        "            torch.save(self.category_vocab, loader_file)\n",
        "            if os.path.exists(self.temp_dir):\n",
        "                shutil.rmtree(self.temp_dir)\n",
        "        for i, category_vocab in self.category_vocab.items():\n",
        "            print(f\"Class {i} category vocabulary: {[self.inv_vocab[w] for w in category_vocab]}\\n\")\n",
        "        \n",
        "\n",
        "    # construct category vocabulary (distributed function)\n",
        "    def category_vocabulary_dist(self, rank, top_pred_num=50, loader_name=\"category_vocab.pt\"):\n",
        "        model = self.model.to(0)\n",
        "        model.eval()\n",
        "        label_name_dataset_loader = self.make_dataloader(rank, self.label_name_data, self.eval_batch_size)\n",
        "        category_words_freq = {i: defaultdict(float) for i in range(self.num_class)}\n",
        "        wrap_label_name_dataset_loader = tqdm(label_name_dataset_loader) if rank == 0 else label_name_dataset_loader\n",
        "        try:\n",
        "            for batch in wrap_label_name_dataset_loader:\n",
        "                with torch.no_grad():\n",
        "                    input_ids = batch[0].to(rank)\n",
        "                    input_mask = batch[1].to(rank)\n",
        "                    label_pos = batch[2].to(rank)\n",
        "                    match_idx = label_pos >= 0\n",
        "                    predictions = model(input_ids,\n",
        "                                        pred_mode=\"mlm\",\n",
        "                                        token_type_ids=None, \n",
        "                                        attention_mask=input_mask)\n",
        "                    _, sorted_res = torch.topk(predictions[match_idx], top_pred_num, dim=-1)\n",
        "                    label_idx = label_pos[match_idx]\n",
        "                    for i, word_list in enumerate(sorted_res):\n",
        "                        for j, word_id in enumerate(word_list):\n",
        "                            category_words_freq[label_idx[i].item()][word_id.item()] += 1\n",
        "            save_file = os.path.join(self.temp_dir, f\"{rank}_\"+loader_name)\n",
        "            torch.save(category_words_freq, save_file)\n",
        "        except RuntimeError as err:\n",
        "            self.cuda_mem_error(err, \"eval\", rank)\n",
        "    \n",
        "    # create dataset loader\n",
        "    def make_dataloader(self, rank, data_dict, batch_size):\n",
        "        if \"labels\" in data_dict:\n",
        "            dataset = TensorDataset(data_dict[\"input_ids\"], data_dict[\"attention_masks\"], data_dict[\"labels\"])\n",
        "        else:\n",
        "            dataset = TensorDataset(data_dict[\"input_ids\"], data_dict[\"attention_masks\"])\n",
        "        sampler = RandomSampler(dataset)\n",
        "        dataset_loader = DataLoader(dataset, sampler=sampler, batch_size=batch_size, shuffle=False)\n",
        "        return dataset_loader    \n",
        " "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skpZ5IAywros",
        "outputId": "46f5c08a-2028-49cb-be63-55baaef5b50c"
      },
      "source": [
        "trainer = LOTClassTrainer(args)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Effective training batch size: 128\n",
            "Label names used for each class are: {0: ['politics'], 1: ['sports'], 2: ['business'], 3: ['technology']}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing LOTClassModel: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing LOTClassModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LOTClassModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LOTClassModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias', 'dense.weight', 'dense.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading encoded texts from ./drive/My Drive/Colab Notebooks/train.pt\n",
            "Loading texts with label names from ./drive/My Drive/Colab Notebooks/label_name_data.pt\n",
            "Loading encoded texts from ./drive/My Drive/Colab Notebooks/test.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNgsDhc7wn8X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5133bd3a-4673-48b8-e68e-b0d75ab3ec96"
      },
      "source": [
        "# Construct category vocabulary\n",
        "trainer.category_vocabulary(top_pred_num=args.top_pred_num, category_vocab_size=args.category_vocab_size)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Contructing category vocabulary.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-5e0a2c1a6982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Construct category vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_pred_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_pred_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-0f66ab5855a3>\u001b[0m in \u001b[0;36mcategory_vocabulary\u001b[0;34m(self, top_pred_num, category_vocab_size, loader_name)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory_vocabulary_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_pred_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mgather_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    197\u001b[0m                ' torch.multiprocessing.start_process(...)' % start_method)\n\u001b[1;32m    198\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstart_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spawn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mdaemon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         )\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0merror_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mprocesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_spawn_posix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mForkServerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_std_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparent_r\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}