{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "pyTorch_Text_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linghduoduo/Deep-Learning/blob/master/pyTorch_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCoJtBrWnGdr",
        "outputId": "a0a5521a-e51b-484f-d5f9-3b689be54b43"
      },
      "source": [
        "!pip3 install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 7.2MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 38.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=02f3ba88d7315b3b6c92086b8bbca7c710626d82ae7fec4321607170b3bf5214\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez2sMGnDn1Pt",
        "outputId": "e7f5c18b-d9b2-4fbe-d958-4fe6e8243f29"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Feb 17 04:45:09 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0ej2wVtpd9F"
      },
      "source": [
        "from collections import defaultdict\n",
        "import time\n",
        "from joblib import Parallel, delayed\n",
        "from multiprocessing import cpu_count\n",
        "from math import ceil"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSLPNWM3ppHe"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.multiprocessing as mp\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import RandomSampler"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rBbZ3h0pvmM",
        "outputId": "75dfdc8d-dc12-4b60-ee4c-a8e99840a206"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lrEsFdwp4B4"
      },
      "source": [
        "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s01YoYoXqAhF"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZeejXZ8qAit",
        "outputId": "472117a1-02c9-48a4-ce49-c31cfb131a9a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "workspace_dir = './drive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vExuE9tJqdG9"
      },
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "args = Namespace(\n",
        "    max_len=200,\n",
        "    dataset_dir=workspace_dir,\n",
        "    label_names_file=\"data/agnews/label_names.txt\",\n",
        "    train_file=\"data/agnews/train.txt\",\n",
        "    test_file=\"data/agnews/test.txt\",\n",
        "    test_label_file=\"data/agnews/test_labels.txt\",\n",
        "    train_batch_size=32,\n",
        "    accum_steps=4,\n",
        "    eval_batch_size=128,\n",
        "    gpus=1,\n",
        "    mcp_epochs=3,\n",
        "    self_train_epochs=1,\n",
        "    dist_port=12345,\n",
        "    update_interval=50,\n",
        "    early_stop=True,\n",
        "    top_pred_num=50,\n",
        "    category_vocab_size=100,\n",
        "    match_threshold=20,\n",
        "    final_model=\"data/agnews/final_model.pt\",\n",
        "    out_file=\"data/agnews/out.txt\"\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmlfHb_gzEOu"
      },
      "source": [
        "from transformers import BertPreTrainedModel, BertModel\n",
        "from transformers.models.bert.modeling_bert import BertOnlyMLMHead\n",
        "from torch import nn\n",
        "import sys"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M93SToy4y-xM"
      },
      "source": [
        "class LOTClassModel(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        self.cls = BertOnlyMLMHead(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        self.init_weights()\n",
        "        # MLM head is not trained\n",
        "        for param in self.cls.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    def forward(self, input_ids, pred_mode, attention_mask=None, token_type_ids=None, \n",
        "                position_ids=None, head_mask=None, inputs_embeds=None):\n",
        "        bert_outputs = self.bert(input_ids,\n",
        "                                 attention_mask=attention_mask,\n",
        "                                 token_type_ids=token_type_ids,\n",
        "                                 position_ids=position_ids,\n",
        "                                 head_mask=head_mask,\n",
        "                                 inputs_embeds=inputs_embeds)\n",
        "        last_hidden_states = bert_outputs[0]\n",
        "        if pred_mode == \"classification\":\n",
        "            trans_states = self.dense(last_hidden_states)\n",
        "            trans_states = self.activation(trans_states)\n",
        "            trans_states = self.dropout(trans_states)\n",
        "            logits = self.classifier(trans_states)\n",
        "        elif pred_mode == \"mlm\":\n",
        "            logits = self.cls(last_hidden_states)\n",
        "        else:\n",
        "            sys.exit(\"Wrong pred_mode!\")\n",
        "        return logits"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ6gNgTbwiZc"
      },
      "source": [
        "class LOTClassTrainer(object):\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.max_len = args.max_len\n",
        "        self.dataset_dir = args.dataset_dir\n",
        "        self.dist_port = args.dist_port\n",
        "        self.num_cpus = min(10, cpu_count() - 1) if cpu_count() > 1 else 1\n",
        "        self.world_size = args.gpus\n",
        "        self.train_batch_size = args.train_batch_size\n",
        "        self.eval_batch_size = args.eval_batch_size\n",
        "        self.accum_steps = args.accum_steps\n",
        "        eff_batch_size = self.train_batch_size * self.world_size * self.accum_steps\n",
        "        assert abs(eff_batch_size - 128) < 10, f\"Make sure the effective training batch size is around 128, current: {eff_batch_size}\"\n",
        "        print(f\"Effective training batch size: {eff_batch_size}\")\n",
        "        self.pretrained_lm = 'bert-base-uncased'\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.pretrained_lm, do_lower_case=True)\n",
        "        self.vocab = self.tokenizer.get_vocab()\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.mask_id = self.vocab[self.tokenizer.mask_token]\n",
        "        self.inv_vocab = {k:v for v, k in self.vocab.items()}\n",
        "        self.read_label_names(args.dataset_dir, args.label_names_file)\n",
        "        self.num_class = len(self.label_name_dict)\n",
        "        self.model = LOTClassModel.from_pretrained(self.pretrained_lm,\n",
        "                                                   output_attentions=False,\n",
        "                                                   output_hidden_states=False,\n",
        "                                                   num_labels=self.num_class)\n",
        "        self.read_data(args.dataset_dir, args.train_file, args.test_file, args.test_label_file)\n",
        "        self.with_test_label = True if args.test_label_file is not None else False\n",
        "        self.temp_dir = f'tmp_{self.dist_port}'\n",
        "        self.mcp_loss = nn.CrossEntropyLoss()\n",
        "        self.st_loss = nn.KLDivLoss(reduction='batchmean')\n",
        "        self.update_interval = args.update_interval\n",
        "        self.early_stop = args.early_stop\n",
        "    \n",
        "    # read label names from file\n",
        "    def read_label_names(self, dataset_dir, label_name_file):\n",
        "        label_name_file = open(os.path.join(dataset_dir, label_name_file))\n",
        "        label_names = label_name_file.readlines()\n",
        "        self.label_name_dict = {i: [word.lower() for word in category_words.strip().split()] for i, category_words in enumerate(label_names)}\n",
        "        print(f\"Label names used for each class are: {self.label_name_dict}\")\n",
        "        self.label2class = {}\n",
        "        self.all_label_name_ids = [self.mask_id]\n",
        "        self.all_label_names = [self.tokenizer.mask_token]\n",
        "        for class_idx in self.label_name_dict:\n",
        "            for word in self.label_name_dict[class_idx]:\n",
        "                assert word not in self.label2class, f\"\\\"{word}\\\" used as the label name by multiple classes!\"\n",
        "                self.label2class[word] = class_idx\n",
        "                if word in self.vocab:\n",
        "                    self.all_label_name_ids.append(self.vocab[word])\n",
        "                    self.all_label_names.append(word)\n",
        "\n",
        "    # read text corpus and labels from files\n",
        "    def read_data(self, dataset_dir, train_file, test_file, test_label_file):\n",
        "        self.train_data, self.label_name_data = self.create_dataset(dataset_dir, train_file, None, \"train.pt\", \n",
        "                                                                    find_label_name=True, label_name_loader_name=\"label_name_data.pt\")\n",
        "        if test_file is not None:\n",
        "            self.test_data = self.create_dataset(dataset_dir, test_file, test_label_file, \"test.pt\")\n",
        "\n",
        "    # convert dataset into tensors\n",
        "    def create_dataset(self, dataset_dir, text_file, label_file, loader_name, find_label_name=False, label_name_loader_name=None):\n",
        "        loader_file = os.path.join(dataset_dir, loader_name)\n",
        "        if os.path.exists(loader_file):\n",
        "            print(f\"Loading encoded texts from {loader_file}\")\n",
        "            data = torch.load(loader_file)\n",
        "        else:\n",
        "            print(f\"Reading texts from {os.path.join(dataset_dir, text_file)}\")\n",
        "            corpus = open(os.path.join(dataset_dir, text_file), encoding=\"utf-8\")\n",
        "            docs = [doc.strip() for doc in corpus.readlines()]\n",
        "            print(f\"Converting texts into tensors.\")\n",
        "            chunk_size = ceil(len(docs) / self.num_cpus)\n",
        "            chunks = [docs[x:x+chunk_size] for x in range(0, len(docs), chunk_size)]\n",
        "            results = Parallel(n_jobs=self.num_cpus)(delayed(self.encode)(docs=chunk) for chunk in chunks)\n",
        "            input_ids = torch.cat([result[0] for result in results])\n",
        "            attention_masks = torch.cat([result[1] for result in results])\n",
        "            print(f\"Saving encoded texts into {loader_file}\")\n",
        "            if label_file is not None:\n",
        "                print(f\"Reading labels from {os.path.join(dataset_dir, label_file)}\")\n",
        "                truth = open(os.path.join(dataset_dir, label_file))\n",
        "                labels = [int(label.strip()) for label in truth.readlines()]\n",
        "                labels = torch.tensor(labels)\n",
        "                data = {\"input_ids\": input_ids, \"attention_masks\": attention_masks, \"labels\": labels}\n",
        "            else:\n",
        "                data = {\"input_ids\": input_ids, \"attention_masks\": attention_masks}\n",
        "            torch.save(data, loader_file)\n",
        "        if find_label_name:\n",
        "            loader_file = os.path.join(dataset_dir, label_name_loader_name)\n",
        "            if os.path.exists(loader_file):\n",
        "                print(f\"Loading texts with label names from {loader_file}\")\n",
        "                label_name_data = torch.load(loader_file)\n",
        "            else:\n",
        "                print(f\"Reading texts from {os.path.join(dataset_dir, text_file)}\")\n",
        "                corpus = open(os.path.join(dataset_dir, text_file), encoding=\"utf-8\")\n",
        "                docs = [doc.strip() for doc in corpus.readlines()]\n",
        "                print(\"Locating label names in the corpus.\")\n",
        "                chunk_size = ceil(len(docs) / self.num_cpus)\n",
        "                chunks = [docs[x:x+chunk_size] for x in range(0, len(docs), chunk_size)]\n",
        "                results = Parallel(n_jobs=self.num_cpus)(delayed(self.label_name_occurrence)(docs=chunk) for chunk in chunks)\n",
        "                input_ids_with_label_name = torch.cat([result[0] for result in results])\n",
        "                attention_masks_with_label_name = torch.cat([result[1] for result in results])\n",
        "                label_name_idx = torch.cat([result[2] for result in results])\n",
        "                assert len(input_ids_with_label_name) > 0, \"No label names appear in corpus!\"\n",
        "                label_name_data = {\"input_ids\": input_ids_with_label_name, \"attention_masks\": attention_masks_with_label_name, \"labels\": label_name_idx}\n",
        "                loader_file = os.path.join(dataset_dir, label_name_loader_name)\n",
        "                print(f\"Saving texts with label names into {loader_file}\")\n",
        "                torch.save(label_name_data, loader_file)\n",
        "            return data, label_name_data\n",
        "        else:\n",
        "            return data\n",
        "\n",
        "    # convert a list of strings to token ids\n",
        "    def encode(self, docs):\n",
        "        encoded_dict = self.tokenizer.batch_encode_plus(docs, add_special_tokens=True, max_length=self.max_len, padding='max_length',\n",
        "                                                        return_attention_mask=True, truncation=True, return_tensors='pt')\n",
        "        input_ids = encoded_dict['input_ids']\n",
        "        attention_masks = encoded_dict['attention_mask']\n",
        "        return input_ids, attention_masks\n",
        "\n",
        "    # convert list of token ids to list of strings\n",
        "    def decode(self, ids):\n",
        "        strings = self.tokenizer.batch_decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "        return strings\n",
        "    \n",
        "    # find label name occurrences in the corpus\n",
        "    def label_name_occurrence(self, docs):\n",
        "        text_with_label = []\n",
        "        label_name_idx = []\n",
        "        for doc in docs:\n",
        "            result = self.label_name_in_doc(doc)\n",
        "            if result is not None:\n",
        "                text_with_label.append(result[0])\n",
        "                label_name_idx.append(result[1].unsqueeze(0))\n",
        "        if len(text_with_label) > 0:\n",
        "            encoded_dict = self.tokenizer.batch_encode_plus(text_with_label, add_special_tokens=True, max_length=self.max_len, \n",
        "                                                            padding='max_length', return_attention_mask=True, truncation=True, return_tensors='pt')\n",
        "            input_ids_with_label_name = encoded_dict['input_ids']\n",
        "            attention_masks_with_label_name = encoded_dict['attention_mask']\n",
        "            label_name_idx = torch.cat(label_name_idx, dim=0)\n",
        "        else:\n",
        "            input_ids_with_label_name = torch.ones(0, self.max_len, dtype=torch.long)\n",
        "            attention_masks_with_label_name = torch.ones(0, self.max_len, dtype=torch.long)\n",
        "            label_name_idx = torch.ones(0, self.max_len, dtype=torch.long)\n",
        "        return input_ids_with_label_name, attention_masks_with_label_name, label_name_idx\n",
        "      \n",
        "    # find label name indices and replace out-of-vocab label names with [MASK]\n",
        "    def label_name_in_doc(self, doc):\n",
        "        doc = self.tokenizer.tokenize(doc)\n",
        "        label_idx = -1 * torch.ones(self.max_len, dtype=torch.long)\n",
        "        new_doc = []\n",
        "        wordpcs = []\n",
        "        idx = 1 # index starts at 1 due to [CLS] token\n",
        "        for i, wordpc in enumerate(doc):\n",
        "            wordpcs.append(wordpc[2:] if wordpc.startswith(\"##\") else wordpc)\n",
        "            if idx >= self.max_len - 1: # last index will be [SEP] token\n",
        "                break\n",
        "            if i == len(doc) - 1 or not doc[i+1].startswith(\"##\"):\n",
        "                word = ''.join(wordpcs)\n",
        "                if word in self.label2class:\n",
        "                    label_idx[idx] = self.label2class[word]\n",
        "                    # replace label names that are not in tokenizer's vocabulary with the [MASK] token\n",
        "                    if word not in self.vocab:\n",
        "                        wordpcs = [self.tokenizer.mask_token]\n",
        "                new_word = ''.join(wordpcs)\n",
        "                if new_word != self.tokenizer.unk_token:\n",
        "                    idx += len(wordpcs)\n",
        "                    new_doc.append(new_word)\n",
        "                wordpcs = []\n",
        "        if (label_idx >= 0).any():\n",
        "            return ' '.join(new_doc), label_idx\n",
        "        else:\n",
        "            return None\n",
        "      \n",
        "    # construct category vocabulary\n",
        "    def category_vocabulary(self, top_pred_num=50, category_vocab_size=100, loader_name=\"category_vocab.pt\"):\n",
        "        loader_file = os.path.join(self.dataset_dir, loader_name)\n",
        "        if os.path.exists(loader_file):\n",
        "            print(f\"Loading category vocabulary from {loader_file}\")\n",
        "            self.category_vocab = torch.load(loader_file)\n",
        "        else:\n",
        "            print(\"Contructing category vocabulary.\")\n",
        "            if not os.path.exists(self.temp_dir):\n",
        "                os.makedirs(self.temp_dir)\n",
        "            # mp.spawn(self.category_vocabulary_dist, nprocs=self.world_size, args=(top_pred_num, loader_name))\n",
        "            self.category_vocabulary_dist(rank=0)\n",
        "            gather_res = []\n",
        "            for f in os.listdir(self.temp_dir):\n",
        "                if f[-3:] == '.pt':\n",
        "                    gather_res.append(torch.load(os.path.join(self.temp_dir, f)))\n",
        "            assert len(gather_res) == self.world_size, \"Number of saved files not equal to number of processes!\"\n",
        "            self.category_words_freq = {i: defaultdict(float) for i in range(self.num_class)}\n",
        "            for i in range(self.num_class):\n",
        "                for category_words_freq in gather_res:\n",
        "                    for word_id, freq in category_words_freq[i].items():\n",
        "                        self.category_words_freq[i][word_id] += freq\n",
        "            self.filter_keywords(category_vocab_size)\n",
        "            torch.save(self.category_vocab, loader_file)\n",
        "            if os.path.exists(self.temp_dir):\n",
        "                shutil.rmtree(self.temp_dir)\n",
        "        for i, category_vocab in self.category_vocab.items():\n",
        "            print(f\"Class {i} category vocabulary: {[self.inv_vocab[w] for w in category_vocab]}\\n\")\n",
        "        \n",
        "\n",
        "    # construct category vocabulary (distributed function)\n",
        "    def category_vocabulary_dist(self, rank, top_pred_num=50, loader_name=\"category_vocab.pt\"):\n",
        "        model = self.model.to(0)\n",
        "        model.eval()\n",
        "        label_name_dataset_loader = self.make_dataloader(rank, self.label_name_data, self.eval_batch_size)\n",
        "        category_words_freq = {i: defaultdict(float) for i in range(self.num_class)}\n",
        "        wrap_label_name_dataset_loader = tqdm(label_name_dataset_loader) if rank == 0 else label_name_dataset_loader\n",
        "        try:\n",
        "            for batch in wrap_label_name_dataset_loader:\n",
        "                with torch.no_grad():\n",
        "                    input_ids = batch[0].to(rank)\n",
        "                    input_mask = batch[1].to(rank)\n",
        "                    label_pos = batch[2].to(rank)\n",
        "                    match_idx = label_pos >= 0\n",
        "                    predictions = model(input_ids,\n",
        "                                        pred_mode=\"mlm\",\n",
        "                                        token_type_ids=None, \n",
        "                                        attention_mask=input_mask)\n",
        "                    _, sorted_res = torch.topk(predictions[match_idx], top_pred_num, dim=-1)\n",
        "                    label_idx = label_pos[match_idx]\n",
        "                    for i, word_list in enumerate(sorted_res):\n",
        "                        for j, word_id in enumerate(word_list):\n",
        "                            category_words_freq[label_idx[i].item()][word_id.item()] += 1\n",
        "            save_file = os.path.join(self.temp_dir, f\"{rank}_\"+loader_name)\n",
        "            torch.save(category_words_freq, save_file)\n",
        "        except RuntimeError as err:\n",
        "            self.cuda_mem_error(err, \"eval\", rank)\n",
        "    \n",
        "    # create dataset loader\n",
        "    def make_dataloader(self, rank, data_dict, batch_size):\n",
        "        if \"labels\" in data_dict:\n",
        "            dataset = TensorDataset(data_dict[\"input_ids\"], data_dict[\"attention_masks\"], data_dict[\"labels\"])\n",
        "        else:\n",
        "            dataset = TensorDataset(data_dict[\"input_ids\"], data_dict[\"attention_masks\"])\n",
        "        sampler = RandomSampler(dataset)\n",
        "        dataset_loader = DataLoader(dataset, sampler=sampler, batch_size=batch_size, shuffle=False)\n",
        "        return dataset_loader\n",
        "\n",
        "    # print error message based on CUDA memory error\n",
        "    def cuda_mem_error(self, err, mode, rank):\n",
        "        if rank == 0:\n",
        "            print(err)\n",
        "            if \"CUDA out of memory\" in str(err):\n",
        "                if mode == \"eval\":\n",
        "                    print(f\"Your GPUs can't hold the current batch size for evaluation, try to reduce `--eval_batch_size`, current: {self.eval_batch_size}\")\n",
        "                else:\n",
        "                    print(f\"Your GPUs can't hold the current batch size for training, try to reduce `--train_batch_size`, current: {self.train_batch_size}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # filter out stop words and words in multiple categories\n",
        "    def filter_keywords(self, category_vocab_size=100):\n",
        "        all_words = defaultdict(list)\n",
        "        sorted_dicts = {}\n",
        "        for i, cat_dict in self.category_words_freq.items():\n",
        "            sorted_dict = {k:v for k, v in sorted(cat_dict.items(), key=lambda item: item[1], reverse=True)[:category_vocab_size]}\n",
        "            sorted_dicts[i] = sorted_dict\n",
        "            for word_id in sorted_dict:\n",
        "                all_words[word_id].append(i)\n",
        "        repeat_words = []\n",
        "        for word_id in all_words:\n",
        "            if len(all_words[word_id]) > 1:\n",
        "                repeat_words.append(word_id)\n",
        "        self.category_vocab = {}\n",
        "        for i, sorted_dict in sorted_dicts.items():\n",
        "            self.category_vocab[i] = np.array(list(sorted_dict.keys()))\n",
        "        stopwords_vocab = stopwords.words('english')\n",
        "        for i, word_list in self.category_vocab.items():\n",
        "            delete_idx = []\n",
        "            for j, word_id in enumerate(word_list):\n",
        "                word = self.inv_vocab[word_id]\n",
        "                if word in self.label_name_dict[i]:\n",
        "                    continue\n",
        "                if not word.isalpha() or len(word) == 1 or word in stopwords_vocab or word_id in repeat_words:\n",
        "                    delete_idx.append(j)\n",
        "            self.category_vocab[i] = np.delete(self.category_vocab[i], delete_idx)\n",
        "\n",
        "    # masked category prediction\n",
        "    def mcp(self, top_pred_num=50, match_threshold=20, epochs=5, loader_name=\"mcp_model.pt\"):\n",
        "        loader_file = os.path.join(self.dataset_dir, loader_name)\n",
        "        if os.path.exists(loader_file):\n",
        "            print(f\"\\nLoading model trained via masked category prediction from {loader_file}\")\n",
        "        else:\n",
        "            self.prepare_mcp(top_pred_num, match_threshold)\n",
        "            print(f\"\\nTraining model via masked category prediction.\")\n",
        "            # mp.spawn(self.mcp_dist, nprocs=self.world_size, args=(epochs, loader_name))\n",
        "            self.mcp_dist(rank=0)\n",
        "        self.model.load_state_dict(torch.load(loader_file))\n",
        "\n",
        "    # prepare self supervision for masked category prediction\n",
        "    def prepare_mcp(self, top_pred_num=50, match_threshold=20, loader_name=\"mcp_train.pt\"):\n",
        "        loader_file = os.path.join(self.dataset_dir, loader_name)\n",
        "        if os.path.exists(loader_file):\n",
        "            print(f\"Loading masked category prediction data from {loader_file}\")\n",
        "            self.mcp_data = torch.load(loader_file)\n",
        "        else:\n",
        "            loader_file = os.path.join(self.dataset_dir, loader_name)\n",
        "            print(\"Preparing self supervision for masked category prediction.\")\n",
        "            if not os.path.exists(self.temp_dir):\n",
        "                os.makedirs(self.temp_dir)\n",
        "            # mp.spawn(self.prepare_mcp_dist, nprocs=self.world_size, args=(top_pred_num, match_threshold, loader_name))\n",
        "            self.prepare_mcp_dist(rank=0)\n",
        "            gather_res = []\n",
        "            for f in os.listdir(self.temp_dir):\n",
        "                if f[-3:] == '.pt':\n",
        "                    gather_res.append(torch.load(os.path.join(self.temp_dir, f)))\n",
        "            assert len(gather_res) == self.world_size, \"Number of saved files not equal to number of processes!\"\n",
        "            all_input_ids = torch.cat([res[\"all_input_ids\"] for res in gather_res], dim=0)\n",
        "            all_mask_label = torch.cat([res[\"all_mask_label\"] for res in gather_res], dim=0)\n",
        "            all_input_mask = torch.cat([res[\"all_input_mask\"] for res in gather_res], dim=0)\n",
        "            category_doc_num = {i: 0 for i in range(self.num_class)}\n",
        "            for i in category_doc_num:\n",
        "                for res in gather_res:\n",
        "                    if i in res[\"category_doc_num\"]:\n",
        "                        category_doc_num[i] += res[\"category_doc_num\"][i]\n",
        "            print(f\"Number of documents with category indicative terms found for each category is: {category_doc_num}\")\n",
        "            self.mcp_data = {\"input_ids\": all_input_ids, \"attention_masks\": all_input_mask, \"labels\": all_mask_label}\n",
        "            torch.save(self.mcp_data, loader_file)\n",
        "            if os.path.exists(self.temp_dir):\n",
        "                shutil.rmtree(self.temp_dir)\n",
        "            for i in category_doc_num:\n",
        "                assert category_doc_num[i] > 10, f\"Too few ({category_doc_num[i]}) documents with category indicative terms found for category {i}; \" \\\n",
        "                       \"try to add more unlabeled documents to the training corpus (recommend) or reduce `--match_threshold` (not recommend)\"\n",
        "        print(f\"There are totally {len(self.mcp_data['input_ids'])} documents with category indicative terms.\")\n",
        " \n",
        "    # prepare self supervision for masked category prediction (distributed function)\n",
        "    def prepare_mcp_dist(self, rank, top_pred_num=50, match_threshold=20, loader_name=\"mcp_train.pt\"):\n",
        "        model = self.model.to(0)\n",
        "        model.eval()\n",
        "        train_dataset_loader = self.make_dataloader(rank, self.train_data, self.eval_batch_size)\n",
        "        all_input_ids = []\n",
        "        all_mask_label = []\n",
        "        all_input_mask = []\n",
        "        category_doc_num = defaultdict(int)\n",
        "        wrap_train_dataset_loader = tqdm(train_dataset_loader) if rank == 0 else train_dataset_loader\n",
        "        try:\n",
        "            for batch in wrap_train_dataset_loader:\n",
        "                with torch.no_grad():\n",
        "                    input_ids = batch[0].to(rank)\n",
        "                    input_mask = batch[1].to(rank)\n",
        "                    predictions = model(input_ids,\n",
        "                                        pred_mode=\"mlm\",\n",
        "                                        token_type_ids=None,\n",
        "                                        attention_mask=input_mask)\n",
        "                    _, sorted_res = torch.topk(predictions, top_pred_num, dim=-1)\n",
        "                    for i, category_vocab in self.category_vocab.items():\n",
        "                        match_idx = torch.zeros_like(sorted_res).bool()\n",
        "                        for word_id in category_vocab:\n",
        "                            match_idx = (sorted_res == word_id) | match_idx\n",
        "                        match_count = torch.sum(match_idx.int(), dim=-1)\n",
        "                        valid_idx = (match_count > match_threshold) & (input_mask > 0)\n",
        "                        valid_doc = torch.sum(valid_idx, dim=-1) > 0\n",
        "                        if valid_doc.any():\n",
        "                            mask_label = -1 * torch.ones_like(input_ids)\n",
        "                            mask_label[valid_idx] = i\n",
        "                            all_input_ids.append(input_ids[valid_doc].cpu())\n",
        "                            all_mask_label.append(mask_label[valid_doc].cpu())\n",
        "                            all_input_mask.append(input_mask[valid_doc].cpu())\n",
        "                            category_doc_num[i] += valid_doc.int().sum().item()\n",
        "            all_input_ids = torch.cat(all_input_ids, dim=0)\n",
        "            all_mask_label = torch.cat(all_mask_label, dim=0)\n",
        "            all_input_mask = torch.cat(all_input_mask, dim=0)\n",
        "            save_dict = {\n",
        "                \"all_input_ids\": all_input_ids,\n",
        "                \"all_mask_label\": all_mask_label,\n",
        "                \"all_input_mask\": all_input_mask,\n",
        "                \"category_doc_num\": category_doc_num,\n",
        "            }\n",
        "            save_file = os.path.join(self.temp_dir, f\"{rank}_\"+loader_name)\n",
        "            torch.save(save_dict, save_file)\n",
        "        except RuntimeError as err:\n",
        "            self.cuda_mem_error(err, \"eval\", rank)\n",
        "\n",
        "    # masked category prediction (distributed function)\n",
        "    def mcp_dist(self, rank, epochs=5, loader_name=\"mcp_model.pt\"):\n",
        "        model = self.model.to(0)\n",
        "        mcp_dataset_loader = self.make_dataloader(rank, self.mcp_data, self.train_batch_size)\n",
        "        total_steps = len(mcp_dataset_loader) * epochs / self.accum_steps\n",
        "        optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5, eps=1e-8)\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*total_steps, num_training_steps=total_steps)\n",
        "        try:\n",
        "            for i in range(epochs):\n",
        "                model.train()\n",
        "                total_train_loss = 0\n",
        "                if rank == 0:\n",
        "                    print(f\"Epoch {i+1}:\")\n",
        "                wrap_mcp_dataset_loader = tqdm(mcp_dataset_loader) if rank == 0 else mcp_dataset_loader\n",
        "                model.zero_grad()\n",
        "                for j, batch in enumerate(wrap_mcp_dataset_loader):\n",
        "                    input_ids = batch[0].to(rank)\n",
        "                    input_mask = batch[1].to(rank)\n",
        "                    labels = batch[2].to(rank)\n",
        "                    mask_pos = labels >= 0\n",
        "                    labels = labels[mask_pos]\n",
        "                    # mask out category indicative words\n",
        "                    input_ids[mask_pos] = self.mask_id\n",
        "                    logits = model(input_ids, \n",
        "                                   pred_mode=\"classification\",\n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=input_mask)\n",
        "                    logits = logits[mask_pos]\n",
        "                    loss = self.mcp_loss(logits.view(-1, self.num_class), labels.view(-1)) / self.accum_steps\n",
        "                    total_train_loss += loss.item()\n",
        "                    loss.backward()\n",
        "                    if (j+1) % self.accum_steps == 0:\n",
        "                        # Clip the norm of the gradients to 1.0.\n",
        "                        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                        optimizer.step()\n",
        "                        scheduler.step()\n",
        "                        model.zero_grad()\n",
        "                avg_train_loss = torch.tensor([total_train_loss / len(mcp_dataset_loader) * self.accum_steps]).to(rank)\n",
        "                # gather_list = [torch.ones_like(avg_train_loss) for _ in range(self.world_size)]\n",
        "                # dist.all_gather(gather_list, avg_train_loss)\n",
        "                # avg_train_loss = torch.tensor(gather_list)\n",
        "                if rank == 0:\n",
        "                    print(f\"Average training loss: {avg_train_loss.mean().item()}\")\n",
        "            if rank == 0:\n",
        "                loader_file = os.path.join(self.dataset_dir, loader_name)\n",
        "                # torch.save(model.module.state_dict(), loader_file)\n",
        "                torch.save(model.state_dict(), loader_file)\n",
        "        except RuntimeError as err:\n",
        "            self.cuda_mem_error(err, \"train\", rank)\n",
        "        \n",
        "    # self training\n",
        "    def self_train(self, epochs, loader_name=\"final_model.pt\"):\n",
        "        loader_file = os.path.join(self.dataset_dir, loader_name)\n",
        "        if os.path.exists(loader_file):\n",
        "            print(f\"\\nFinal model {loader_file} found, skip self-training\")\n",
        "        else:\n",
        "            rand_idx = torch.randperm(len(self.train_data[\"input_ids\"]))\n",
        "            self.train_data = {\"input_ids\": self.train_data[\"input_ids\"][rand_idx],\n",
        "                               \"attention_masks\": self.train_data[\"attention_masks\"][rand_idx]}\n",
        "            print(f\"\\nStart self-training.\")\n",
        "            # mp.spawn(self.self_train_dist, nprocs=self.world_size, args=(epochs, loader_name))\n",
        "            self.self_train_dist(rank=0)\n",
        "    \n",
        "    # self training (distributed function)\n",
        "    def self_train_dist(self, rank, epochs=1, loader_name=\"final_model.pt\"):\n",
        "        model = self.model.to(0)\n",
        "        test_dataset_loader = self.make_dataloader(rank, self.test_data, self.eval_batch_size) if self.with_test_label else None\n",
        "        total_steps = int(len(self.train_data[\"input_ids\"]) * epochs / (self.world_size * self.train_batch_size * self.accum_steps))\n",
        "        optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-6, eps=1e-8)\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*total_steps, num_training_steps=total_steps)\n",
        "        idx = 0\n",
        "        if self.early_stop:\n",
        "            agree_count = 0\n",
        "        for i in range(int(total_steps / self.update_interval)):\n",
        "            self_train_dict, idx, agree = self.prepare_self_train_data(rank, model, idx)\n",
        "            # early stop if current prediction agrees with target distribution for 3 consecutive updates\n",
        "            if self.early_stop:\n",
        "                if 1 - agree < 1e-3:\n",
        "                    agree_count += 1\n",
        "                else:\n",
        "                    agree_count = 0\n",
        "                if agree_count >= 3:\n",
        "                    break\n",
        "            self_train_dataset_loader = self.make_dataloader(rank, self_train_dict, self.train_batch_size)\n",
        "            self.self_train_batches(rank, model, self_train_dataset_loader, optimizer, scheduler, test_dataset_loader)\n",
        "        if rank == 0:\n",
        "            loader_file = os.path.join(self.dataset_dir, loader_name)\n",
        "            print(f\"Saving final model to {loader_file}\")\n",
        "            # torch.save(model.module.state_dict(), loader_file)\n",
        "            torch.save(model.state_dict(), loader_file)\n",
        "\n",
        "    # prepare self training data and target distribution\n",
        "    def prepare_self_train_data(self, rank, model, idx):\n",
        "        target_num = min(self.world_size * self.train_batch_size * self.update_interval * self.accum_steps, len(self.train_data[\"input_ids\"]))\n",
        "        if idx + target_num >= len(self.train_data[\"input_ids\"]):\n",
        "            select_idx = torch.cat((torch.arange(idx, len(self.train_data[\"input_ids\"])),\n",
        "                                    torch.arange(idx + target_num - len(self.train_data[\"input_ids\"]))))\n",
        "        else:\n",
        "            select_idx = torch.arange(idx, idx + target_num)\n",
        "        assert len(select_idx) == target_num\n",
        "        idx = (idx + len(select_idx)) % len(self.train_data[\"input_ids\"])\n",
        "        select_dataset = {\"input_ids\": self.train_data[\"input_ids\"][select_idx],\n",
        "                          \"attention_masks\": self.train_data[\"attention_masks\"][select_idx]}\n",
        "        dataset_loader = self.make_dataloader(rank, select_dataset, self.eval_batch_size)\n",
        "        input_ids, input_mask, preds = self.inference(model, dataset_loader, rank, return_type=\"data\")\n",
        "        # gather_input_ids = [torch.ones_like(input_ids) for _ in range(self.world_size)]\n",
        "        # gather_input_mask = [torch.ones_like(input_mask) for _ in range(self.world_size)]\n",
        "        # gather_preds = [torch.ones_like(preds) for _ in range(self.world_size)]\n",
        "        # dist.all_gather(gather_input_ids, input_ids)\n",
        "        # dist.all_gather(gather_input_mask, input_mask)\n",
        "        # dist.all_gather(gather_preds, preds)\n",
        "        # input_ids = torch.cat(gather_input_ids, dim=0).cpu()\n",
        "        # input_mask = torch.cat(gather_input_mask, dim=0).cpu()\n",
        "        # all_preds = torch.cat(gather_preds, dim=0).cpu()\n",
        "        input_ids = torch.tensor(input_ids).to(rank)\n",
        "        input_mask = torch.tensor(input_mask).to(rank)\n",
        "        preds = torch.tensor(preds).to(rank)\n",
        "        all_preds = preds\n",
        "        weight = all_preds**2 / torch.sum(all_preds, dim=0)\n",
        "        target_dist = (weight.t() / torch.sum(weight, dim=1)).t()\n",
        "        all_target_pred = target_dist.argmax(dim=-1)\n",
        "        agree = (all_preds.argmax(dim=-1) == all_target_pred).int().sum().item() / len(all_target_pred)\n",
        "        self_train_dict = {\"input_ids\": input_ids, \"attention_masks\": input_mask, \"labels\": target_dist}\n",
        "        return self_train_dict, idx, agree\n",
        "\n",
        "    # use a model to do inference on a dataloader\n",
        "    def inference(self, model, dataset_loader, rank, return_type):\n",
        "        if return_type == \"data\":\n",
        "            all_input_ids = []\n",
        "            all_input_mask = []\n",
        "            all_preds = []\n",
        "        elif return_type == \"acc\":\n",
        "            pred_labels = []\n",
        "            truth_labels = []\n",
        "        elif return_type == \"pred\":\n",
        "            pred_labels = []\n",
        "        model.eval()\n",
        "        try:\n",
        "            for batch in dataset_loader:\n",
        "                with torch.no_grad():\n",
        "                    input_ids = batch[0].to(rank)\n",
        "                    input_mask = batch[1].to(rank)\n",
        "                    logits = model(input_ids,\n",
        "                                   pred_mode=\"classification\",\n",
        "                                   token_type_ids=None,\n",
        "                                   attention_mask=input_mask)\n",
        "                    logits = logits[:, 0, :]\n",
        "                    if return_type == \"data\":\n",
        "                        all_input_ids.append(input_ids)\n",
        "                        all_input_mask.append(input_mask)\n",
        "                        all_preds.append(nn.Softmax(dim=-1)(logits))\n",
        "                    elif return_type == \"acc\":\n",
        "                        labels = batch[2]\n",
        "                        pred_labels.append(torch.argmax(logits, dim=-1).cpu())\n",
        "                        truth_labels.append(labels)\n",
        "                    elif return_type == \"pred\":\n",
        "                        pred_labels.append(torch.argmax(logits, dim=-1).cpu())\n",
        "            if return_type == \"data\":\n",
        "                all_input_ids = torch.cat(all_input_ids, dim=0)\n",
        "                all_input_mask = torch.cat(all_input_mask, dim=0)\n",
        "                all_preds = torch.cat(all_preds, dim=0)\n",
        "                return all_input_ids, all_input_mask, all_preds\n",
        "            elif return_type == \"acc\":\n",
        "                pred_labels = torch.cat(pred_labels, dim=0)\n",
        "                truth_labels = torch.cat(truth_labels, dim=0)\n",
        "                samples = len(truth_labels)\n",
        "                acc = (pred_labels == truth_labels).float().sum() / samples\n",
        "                return acc.to(rank)\n",
        "            elif return_type == \"pred\":\n",
        "                pred_labels = torch.cat(pred_labels, dim=0)\n",
        "                return pred_labels\n",
        "        except RuntimeError as err:\n",
        "            self.cuda_mem_error(err, \"eval\", rank)\n",
        "\n",
        "    # train a model on batches of data with target labels\n",
        "    def self_train_batches(self, rank, model, self_train_loader, optimizer, scheduler, test_dataset_loader):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        wrap_train_dataset_loader = tqdm(self_train_loader) if rank == 0 else self_train_loader\n",
        "        model.zero_grad()\n",
        "        try:\n",
        "            for j, batch in enumerate(wrap_train_dataset_loader):\n",
        "                input_ids = batch[0].to(rank)\n",
        "                input_mask = batch[1].to(rank)\n",
        "                target_dist = batch[2].to(rank)\n",
        "                logits = model(input_ids,\n",
        "                               pred_mode=\"classification\",\n",
        "                               token_type_ids=None,\n",
        "                               attention_mask=input_mask)\n",
        "                logits = logits[:, 0, :]\n",
        "                preds = nn.LogSoftmax(dim=-1)(logits)\n",
        "                loss = self.st_loss(preds.view(-1, self.num_class), target_dist.view(-1, self.num_class)) / self.accum_steps\n",
        "                total_train_loss += loss.item()\n",
        "                loss.backward()\n",
        "                if (j+1) % self.accum_steps == 0:\n",
        "                    # Clip the norm of the gradients to 1.0.\n",
        "                    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    model.zero_grad()\n",
        "            if self.with_test_label:\n",
        "                acc = self.inference(model, test_dataset_loader, rank, return_type=\"acc\")\n",
        "                acc = torch.tensor(acc).to(rank).mean().item()\n",
        "                # gather_acc = [torch.ones_like(acc) for _ in range(self.world_size)]\n",
        "                # dist.all_gather(gather_acc, acc)\n",
        "                # acc = torch.tensor(gather_acc).mean().item()\n",
        "            avg_train_loss = torch.tensor([total_train_loss / len(wrap_train_dataset_loader) * self.accum_steps]).to(rank)\n",
        "            # gather_list = [torch.ones_like(avg_train_loss) for _ in range(self.world_size)]\n",
        "            # dist.all_gather(gather_list, avg_train_loss)\n",
        "            # avg_train_loss = torch.tensor(gather_list)\n",
        "            if rank == 0:\n",
        "                print(f\"lr: {optimizer.param_groups[0]['lr']:.4g}\")\n",
        "                print(f\"Average training loss: {avg_train_loss.mean().item()}\")\n",
        "                if self.with_test_label:\n",
        "                    print(f\"Test acc: {acc}\")\n",
        "        except RuntimeError as err:\n",
        "            self.cuda_mem_error(err, \"train\", rank)\n",
        "\n",
        "    # use trained model to make predictions on the test set\n",
        "    def write_results(self, loader_name=\"final_model.pt\", out_file=\"out.txt\"):\n",
        "        loader_file = os.path.join(self.dataset_dir, loader_name)\n",
        "        assert os.path.exists(loader_file)\n",
        "        print(f\"\\nLoading final model from {loader_file}\")\n",
        "        self.model.load_state_dict(torch.load(loader_file))\n",
        "        self.model.to(0)\n",
        "        test_set = TensorDataset(self.test_data[\"input_ids\"], self.test_data[\"attention_masks\"])\n",
        "        test_dataset_loader = DataLoader(test_set, sampler=SequentialSampler(test_set), batch_size=self.eval_batch_size)\n",
        "        pred_labels = self.inference(self.model, test_dataset_loader, 0, return_type=\"pred\")\n",
        "        out_file = os.path.join(self.dataset_dir, out_file)\n",
        "        print(f\"Writing prediction results to {out_file}\")\n",
        "        f_out = open(out_file, 'w')\n",
        "        for label in pred_labels:\n",
        "            f_out.write(str(label.item()) + '\\n')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skpZ5IAywros",
        "outputId": "d97d7052-eef7-4990-8067-ae8bf7a9bc6b"
      },
      "source": [
        "trainer = LOTClassTrainer(args)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Effective training batch size: 128\n",
            "Label names used for each class are: {0: ['politics'], 1: ['sports'], 2: ['business'], 3: ['technology']}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing LOTClassModel: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing LOTClassModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LOTClassModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LOTClassModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias', 'dense.weight', 'dense.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading encoded texts from ./drive/My Drive/Colab Notebooks/train.pt\n",
            "Loading texts with label names from ./drive/My Drive/Colab Notebooks/label_name_data.pt\n",
            "Loading encoded texts from ./drive/My Drive/Colab Notebooks/test.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNgsDhc7wn8X",
        "outputId": "8e2e4988-abab-4e3d-a2b9-9aa5de443e5e"
      },
      "source": [
        "# Construct category vocabulary\n",
        "trainer.category_vocabulary(top_pred_num=args.top_pred_num, category_vocab_size=args.category_vocab_size)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading category vocabulary from ./drive/My Drive/Colab Notebooks/category_vocab.pt\n",
            "Class 0 category vocabulary: ['politics', 'political', 'politicians', 'government', 'elections', 'politician', 'democracy', 'democratic', 'governing', 'party', 'state', 'leadership', 'election', 'politically', 'affairs', 'issues', 'governments', 'voters', 'debate', 'cabinet', 'congress', 'democrat', 'administration', 'president', 'religion', 'republican', 'history', 'war', 'crisis', 'legislature', 'governance', 'candidates', 'opposition', 'pr', 'problems', 'relations', 'justice', 'finance', 'struggle', 'rhetoric', 'right', 'convention', 'votes', 'fighting', 'violence', 'senate', 'matters', 'fight', 'republicans', 'trouble', 'parliament', 'us', 'one', 'conflict', 'soil', 'voting', 'law', 'parliamentary', 'representation', 'reality', 'house', 'campaign', 'wars', 'candidate', 'contest', 'campaigns', 'legislative', 'transition', 'labor']\n",
            "\n",
            "Class 1 category vocabulary: ['sports', 'games', 'sporting', 'athletics', 'game', 'national', 'news', 'athletic', 'espn', 'soccer', 'stadium', 'basketball', 'arts', 'racing', 'baseball', 'tv', 'hockey', 'pro', 'press', 'team', 'red', 'home', 'bay', 'kings', 'legends', 'city', 'winning', 'miracle', 'olympic', 'go', 'giants', 'champions', 'ball', 'players', 'prime', 'boxing', 'teams', 'athletes', 'tennis', 'club', 'blue', 'coaches', 'gold', 'west', 'toronto', 'classic', 'pittsburgh', 'super', 'nfl', 'magic', 'key', 'times', 'field', 'rogers', 'warriors', 'stars', 'gym', 'championship', 'losses', 'college', 'mlb', 'veterans', 'rugby', 'hits', 'bc', 'sun', 'events', 'south', 'nba']\n",
            "\n",
            "Class 2 category vocabulary: ['business', 'businesses', 'trade', 'commercial', 'enterprise', 'shop', 'money', 'market', 'commerce', 'corporate', 'global', 'future', 'sales', 'general', 'group', 'retail', 'companies', 'management', 'operations', 'operation', 'corporation', 'store', 'division', 'firm', 'venture', 'brand', 'contract', 'revenue', 'economic', 'branch', 'subsidiary', 'personal', 'cash', 'short', 'line', 'bank', 'customer', 'concern', 'growth', 'chain', 'strategic', 'family', 'work', 'products', 'big', 'scientific', 'virtual', 'engineering', 'sector', 'trading', 'portfolio', 'ceo', 'segment', 'investment', 'working', 'executive', 'private', 'services', 'public', 'job', 'marketing']\n",
            "\n",
            "Class 3 category vocabulary: ['technology', 'technologies', 'tech', 'software', 'technological', 'device', 'equipment', 'hardware', 'infrastructure', 'devices', 'system', 'knowledge', 'technique', 'digital', 'technical', 'concept', 'systems', 'gear', 'techniques', 'functionality', 'material', 'process', 'facility', 'feature', 'capability', 'content', 'method', 'security', 'ability', 'network', 'internet', 'computing', 'chip', 'smart', 'modern', 'communication', 'language', 'mechanism', 'computer', 'design', 'cyber', 'standard', 'tool', 'development', 'format', 'protocol', 'wireless', 'phone', 'information', 'program', 'ce', 'plant', 'large', 'data', 'project', 'application', 'theory', 'science', 'performance', 'common', 'ict', 'os', 'speed', 'sensor', 'capabilities', 'electronic', 'society', 'silicon', 'memory', 'invention']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl3XzGStb7qp",
        "outputId": "eea79346-4cea-4ed7-dcb4-c52442d5c0a9"
      },
      "source": [
        "# Training with masked category prediction\n",
        "trainer.mcp(top_pred_num=args.top_pred_num, match_threshold=args.match_threshold, epochs=args.mcp_epochs)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading model trained via masked category prediction from ./drive/My Drive/Colab Notebooks/mcp_model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLc6HVEJrZXo",
        "outputId": "65129534-8552-42fb-b23a-419d9ba6bd3c"
      },
      "source": [
        "# Self-training \n",
        "trainer.self_train(epochs=args.self_train_epochs, loader_name=args.final_model)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Final model ./drive/My Drive/Colab Notebooks/final_model.pt found, skip self-training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lr4WYzuUiYZ",
        "outputId": "9312f5c2-6554-4662-c89b-7b954849048a"
      },
      "source": [
        "# Write test set results\n",
        "if args.test_file is not None:\n",
        "    trainer.write_results(loader_name=args.final_model, out_file=args.out_file)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading final model from ./drive/My Drive/Colab Notebooks/final_model.pt\n",
            "Writing prediction results to ./drive/My Drive/Colab Notebooks/out.txt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}