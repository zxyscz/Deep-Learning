{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Examples.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNX+M5QubxnSTLwitmEBsw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linghduoduo/Deep-Learning/blob/master/RNN_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkN_bkkyKh6T"
      },
      "source": [
        "https://towardsdatascience.com/implementing-recurrent-neural-network-using-numpy-c359a0a68a67"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYlBHb1IIj2o"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "hidden_dim = 100       \n",
        "output_dim = 80 # this is the total unique words in the vocabulary\n",
        "input_weights = np.random.uniform(0, 1, (hidden_dim,hidden_dim))\n",
        "internal_state_weights = np.random.uniform(0,1, (hidden_dim, hidden_dim))\n",
        "output_weights = np.random.uniform(0,1, (output_dim,hidden_dim))\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsSbl4XxIwcA"
      },
      "source": [
        "prev_memory =  np.zeros((hidden_dim,1))\n",
        "learning_rate = 0.0001    \n",
        "nepoch = 25               \n",
        "T = 4   # length of sequence\n",
        "bptt_truncate = 2 \n",
        "dU = np.zeros(input_weights.shape)\n",
        "dV = np.zeros(output_weights.shape)\n",
        "dW = np.zeros(internal_state_weights.shape)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "768dkBuYI-B_"
      },
      "source": [
        "input_string = [2,45,10,65]\n",
        "embeddings = [] # this is the sentence embedding list that contains the embeddings for each word\n",
        "for i in range(0,T):\n",
        "    x = np.random.randn(hidden_dim,1)\n",
        "    embeddings.append(x)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGboVOB6LFjK",
        "outputId": "c71712ae-18f0-4fa8-ec98-1b74a3c8d24b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "output_mapper = {}\n",
        "for index_value in output_string :\n",
        "    output_mapper[index_value]  = identity_matrix[index_value,:]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-eef57661a856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_string\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0moutput_mapper\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_value\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0midentity_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'output_string' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBusS4k0Lb0L"
      },
      "source": [
        "output_t = {}\n",
        "i=0\n",
        "for key,value in output_mapper.items():\n",
        "    output_t[i] = value\n",
        "    i+=1;"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abpOE8k3JJA0"
      },
      "source": [
        "def tanh_activation(Z):\n",
        "     return (np.exp(Z)-np.exp(-Z))/(np.exp(Z)-np.exp(-Z)) # this is the tanh function can also be written as np.tanh(Z)\n",
        "     \n",
        "def softmax_activation(Z):\n",
        "        e_x = np.exp(Z - np.max(Z))  # this is the code for softmax function \n",
        "        return e_x / e_x.sum(axis=0) \n",
        "\n",
        "def Rnn_forward(input_embedding, input_weights, internal_state_weights, prev_memory,output_weights):\n",
        "    forward_params = []\n",
        "    W_frd = np.dot(internal_state_weights,prev_memory)\n",
        "    U_frd = np.dot(input_weights,input_embedding)\n",
        "    sum_s = W_frd + U_frd\n",
        "    ht_activated = tanh_activation(sum_s)\n",
        "    yt_unactivated = np.asarray(np.dot(output_weights,  tanh_activation(sum_s)))\n",
        "    yt_activated = softmax_activation(yt_unactivated)\n",
        "    forward_params.append([W_frd,U_frd,sum_s,yt_unactivated])\n",
        "    return ht_activated,yt_activated,forward_params"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utRHHaxiJYtY"
      },
      "source": [
        "def calculate_loss(output_mapper,predicted_output):\n",
        "    total_loss = 0\n",
        "    layer_loss = []\n",
        "    for y,y_ in zip(output_mapper.values(),predicted_output): # this for loop calculation is for the first equation, where loss for each time-stamp is calculated\n",
        "        loss = -sum(y[i]*np.log2(y_[i]) for i in range(len(y)))\n",
        "        loss = loss/ float(len(y))\n",
        "        layer_loss.append(loss) \n",
        "    for i in range(len(layer_loss)): #this the total loss calculated for all the time-stamps considered together. \n",
        "        total_loss  = total_loss + layer_loss[i]\n",
        "    return total_loss/float(len(predicted_output))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSEanBvDJfBE"
      },
      "source": [
        "def delta_cross_entropy(predicted_output,original_t_output):\n",
        "    li = []\n",
        "    grad = predicted_output\n",
        "    for i,l in enumerate(original_t_output): #check if the value in the index is 1 or not, if yes then take the same index value from the predicted_ouput list and subtract 1 from it. \n",
        "        if l == 1:\n",
        "    #grad = np.asarray(np.concatenate( grad, axis=0 ))\n",
        "            grad[i] -= 1\n",
        "    return grad"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMG5v4RLJoI7"
      },
      "source": [
        "def multiplication_backward(weights,x,dz):\n",
        "    gradient_weight = np.array(np.dot(np.asmatrix(dz),np.transpose(np.asmatrix(x))))\n",
        "    chain_gradient = np.dot(np.transpose(weights),dz)\n",
        "    return gradient_weight,chain_gradient\n",
        "\n",
        "def add_backward(x1,x2,dz):    # this function is for calculating the derivative of ht_unactivated function\n",
        "    dx1 = dz * np.ones_like(x1)\n",
        "    dx2 = dz * np.ones_like(x2)\n",
        "    return dx1,dx2\n",
        "\n",
        "def tanh_activation_backward(x,top_diff):\n",
        "    output = np.tanh(x)\n",
        "    return (1.0 - np.square(output)) * top_diff"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvg_49OZJxxt"
      },
      "source": [
        "def single_backprop(X,input_weights,internal_state_weights,output_weights,ht_activated,dLo,forward_params_t,diff_s,prev_s):# inlide all the param values for all the data thats there\n",
        "    W_frd = forward_params_t[0][0] \n",
        "    U_frd = forward_params_t[0][1]\n",
        "    ht_unactivated = forward_params_t[0][2]\n",
        "    yt_unactivated = forward_params_t[0][3]\n",
        "    dV,dsv = multiplication_backward(output_weights,ht_activated,dLo)\n",
        "    ds = np.add(dsv,diff_s) # used for truncation of memory \n",
        "    dadd = tanh_activation_backward(ht_unactivated, ds)\n",
        "    dmulw,dmulu = add_backward(U_frd,W_frd,dadd)\n",
        "    dW, dprev_s = multiplication_backward(internal_state_weights, prev_s ,dmulw)\n",
        "    dU, dx = multiplication_backward(input_weights, X, dmulu) #input weights\n",
        "    return (dprev_s, dU, dW, dV)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE3212xSJ59W"
      },
      "source": [
        "def rnn_backprop(embeddings,memory,output_t,dU,dV,dW,bptt_truncate,input_weights,output_weights,internal_state_weights):\n",
        "    T = 4\n",
        "    # we start the backprop from the first timestamp. \n",
        "    for t in range(4):\n",
        "        prev_s_t = np.zeros((hidden_dim,1)) #required as the first timestamp does not have a previous memory, \n",
        "        diff_s = np.zeros((hidden_dim,1)) # this is used for the truncating purpose of restoring a previous information from the before level\n",
        "        predictions = memory[\"yt\" + str(t)]\n",
        "        ht_activated = memory[\"ht\" + str(t)]\n",
        "        forward_params_t = memory[\"params\"+ str(t)] \n",
        "        dLo = delta_cross_entropy(predictions,output_t[t]) #the loss derivative for that particular timestamp\n",
        "        dprev_s, dU_t, dW_t, dV_t = single_backprop(embeddings[t],input_weights,internal_state_weights,output_weights,ht_activated,dLo,forward_params_t,diff_s,prev_s_t)\n",
        "        prev_s_t = ht_activated\n",
        "        prev = t-1\n",
        "        dLo = np.zeros((output_dim,1)) #here the loss deriative is turned to 0 as we do not require it for the turncated information.\n",
        "        # the following code is for the trunated bptt and its for each time-stamp. \n",
        "        for i in range(t-1,max(-1,t-bptt_truncate),-1):\n",
        "            forward_params_t = memory[\"params\" + str(i)]\n",
        "            ht_activated = memory[\"ht\" + str(i)]\n",
        "            prev_s_i = np.zeros((hidden_dim,1)) if i == 0 else memory[\"ht\" + str(prev)]\n",
        "            dprev_s, dU_i, dW_i, dV_i = single_backprop(embeddings[t] ,input_weights,internal_state_weights,output_weights,ht_activated,dLo,forward_params_t,dprev_s,prev_s_i)\n",
        "            dU_t += dU_i #adding the previous gradients on lookback to the current time sequence \n",
        "            dW_t += dW_i\n",
        "        dV += dV_t \n",
        "        dU += dU_t\n",
        "        dW += dW_t\n",
        "    return (dU, dW, dV)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCgQpCReJ-6I"
      },
      "source": [
        "def gd_step(learning_rate, dU,dW,dV, input_weights, internal_state_weights,output_weights ):\n",
        "    input_weights -= learning_rate* dU\n",
        "    internal_state_weights -= learning_rate * dW\n",
        "    output_weights -=learning_rate * dV\n",
        "    return input_weights,internal_state_weights,output_weights"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUXx9an7KFma",
        "outputId": "d26f2125-b267-483f-fa6f-f647134bbfb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "def train(T, embeddings,output_t,output_mapper,input_weights,internal_state_weights,output_weights,dU,dW,dV,prev_memory,learning_rate=0.001, nepoch=100, evaluate_loss_after=2):\n",
        "    losses = []\n",
        "    for epoch in range(nepoch):\n",
        "        if(epoch % evaluate_loss_after == 0):\n",
        "                output_string,memory = full_forward_prop(T, embeddings ,input_weights,internal_state_weights,prev_memory,output_weights)\n",
        "                loss = calculate_loss(output_mapper, output_string)\n",
        "                losses.append(loss)\n",
        "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                print(\"%s: Loss after  epoch=%d: %f\" % (time,epoch, loss))\n",
        "                sys.stdout.flush()\n",
        "        dU,dW,dV = rnn_backprop(embeddings,memory,output_t,dU,dV,dW,bptt_truncate,input_weights,output_weights,internal_state_weights)\n",
        "        input_weights,internal_state_weights,output_weights= sgd_step(learning_rate,dU,dW,dV,input_weights,internal_state_weights,output_weights)\n",
        "    return losses\n",
        "\n",
        "losses = train(T, embeddings,output_t,output_mapper,input_weights,internal_state_weights,output_weights,dU,dW,dV,prev_memory,learning_rate=0.0001, nepoch=10, evaluate_loss_after=2)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-62cb1e53bf98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_mapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minternal_state_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprev_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_loss_after\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'output_t' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knf7cFqNKJmB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}