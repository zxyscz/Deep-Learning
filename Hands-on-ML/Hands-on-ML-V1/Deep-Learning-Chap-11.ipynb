{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def logit(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEJCAYAAACdePCvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1087c3240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier and He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEJCAYAAAC0U81tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11899bb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z<0, alpha*(np.exp(z)-1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAELCAYAAADN4q16AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEfP/V8w0Ngt2SJkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118a0f9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train MNIST using Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\", reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.6 Test accuracy: 0.6653\n",
      "1 Train accuracy: 0.7 Test accuracy: 0.7691\n",
      "2 Train accuracy: 0.84 Test accuracy: 0.8278\n",
      "3 Train accuracy: 0.85 Test accuracy: 0.8566\n",
      "4 Train accuracy: 0.9 Test accuracy: 0.8709\n",
      "5 Train accuracy: 0.78 Test accuracy: 0.8812\n",
      "6 Train accuracy: 0.88 Test accuracy: 0.8858\n",
      "7 Train accuracy: 0.87 Test accuracy: 0.8912\n",
      "8 Train accuracy: 0.85 Test accuracy: 0.8936\n",
      "9 Train accuracy: 0.89 Test accuracy: 0.8991\n",
      "10 Train accuracy: 0.86 Test accuracy: 0.902\n",
      "11 Train accuracy: 0.88 Test accuracy: 0.9029\n",
      "12 Train accuracy: 0.89 Test accuracy: 0.9069\n",
      "13 Train accuracy: 0.92 Test accuracy: 0.9093\n",
      "14 Train accuracy: 0.95 Test accuracy: 0.9101\n",
      "15 Train accuracy: 0.92 Test accuracy: 0.913\n",
      "16 Train accuracy: 0.9 Test accuracy: 0.9141\n",
      "17 Train accuracy: 0.89 Test accuracy: 0.9169\n",
      "18 Train accuracy: 0.93 Test accuracy: 0.9179\n",
      "19 Train accuracy: 0.91 Test accuracy: 0.9185\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(mnist.test.labels)//batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## tf.layers.batch_normalization\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "from functools import partial\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ELU activation function and Batch Normalization at each layer:\n",
    "tf.reset_default_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    \n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    \n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9182\n",
      "1 Test accuracy: 0.9395\n",
      "2 Test accuracy: 0.9506\n",
      "3 Test accuracy: 0.9588\n",
      "4 Test accuracy: 0.9607\n",
      "5 Test accuracy: 0.9671\n",
      "6 Test accuracy: 0.9673\n",
      "7 Test accuracy: 0.9678\n",
      "8 Test accuracy: 0.9719\n",
      "9 Test accuracy: 0.9723\n",
      "10 Test accuracy: 0.9736\n",
      "11 Test accuracy: 0.9732\n",
      "12 Test accuracy: 0.9734\n",
      "13 Test accuracy: 0.9742\n",
      "14 Test accuracy: 0.9761\n",
      "15 Test accuracy: 0.9758\n",
      "16 Test accuracy: 0.9769\n",
      "17 Test accuracy: 0.9767\n",
      "18 Test accuracy: 0.9777\n",
      "19 Test accuracy: 0.9773\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/gamma:0']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1:0\", shape=(784, 300), dtype=float32)\n",
      "<tf.Variable 'hidden1/kernel:0' shape=(784, 300) dtype=float32_ref>\n",
      "Tensor(\"gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1:0\", shape=(300,), dtype=float32)\n",
      "<tf.Variable 'hidden1/bias:0' shape=(300,) dtype=float32_ref>\n",
      "Tensor(\"gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1:0\", shape=(300, 50), dtype=float32)\n",
      "<tf.Variable 'hidden2/kernel:0' shape=(300, 50) dtype=float32_ref>\n",
      "Tensor(\"gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1:0\", shape=(50,), dtype=float32)\n",
      "<tf.Variable 'hidden2/bias:0' shape=(50,) dtype=float32_ref>\n",
      "Tensor(\"gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1:0\", shape=(50, 50), dtype=float32)\n",
      "<tf.Variable 'hidden3/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "Tensor(\"gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1:0\", shape=(50,), dtype=float32)\n",
      "<tf.Variable 'hidden3/bias:0' shape=(50,) dtype=float32_ref>\n",
      "Tensor(\"gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1:0\", shape=(50, 50), dtype=float32)\n",
      "<tf.Variable 'hidden4/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "Tensor(\"gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1:0\", shape=(50,), dtype=float32)\n",
      "<tf.Variable 'hidden4/bias:0' shape=(50,) dtype=float32_ref>\n",
      "Tensor(\"gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1:0\", shape=(50, 50), dtype=float32)\n",
      "<tf.Variable 'hidden5/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "Tensor(\"gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1:0\", shape=(50,), dtype=float32)\n",
      "<tf.Variable 'hidden5/bias:0' shape=(50,) dtype=float32_ref>\n",
      "Tensor(\"gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1:0\", shape=(50, 10), dtype=float32)\n",
      "<tf.Variable 'outputs/kernel:0' shape=(50, 10) dtype=float32_ref>\n",
      "Tensor(\"gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1:0\", shape=(10,), dtype=float32)\n",
      "<tf.Variable 'outputs/bias:0' shape=(10,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "## tf.clip_by_value\n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "\n",
    "for grad, var in grads_and_vars:\n",
    "    print(grad)\n",
    "    print(var)\n",
    "    \n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.4701\n",
      "1 Test accuracy: 0.7807\n",
      "2 Test accuracy: 0.8699\n",
      "3 Test accuracy: 0.8904\n",
      "4 Test accuracy: 0.9099\n",
      "5 Test accuracy: 0.9173\n",
      "6 Test accuracy: 0.9243\n",
      "7 Test accuracy: 0.9273\n",
      "8 Test accuracy: 0.9293\n",
      "9 Test accuracy: 0.9363\n",
      "10 Test accuracy: 0.9368\n",
      "11 Test accuracy: 0.9396\n",
      "12 Test accuracy: 0.9411\n",
      "13 Test accuracy: 0.9463\n",
      "14 Test accuracy: 0.9488\n",
      "15 Test accuracy: 0.9514\n",
      "16 Test accuracy: 0.9521\n",
      "17 Test accuracy: 0.955\n",
      "18 Test accuracy: 0.9522\n",
      "19 Test accuracy: 0.9562\n"
     ]
    }
   ],
   "source": [
    "## Unable to run\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing a TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel\n",
      "hidden3/kernel/Assign\n",
      "hidden3/kernel/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias\n",
      "hidden3/bias/Assign\n",
      "hidden3/bias/read\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel\n",
      "hidden4/kernel/Assign\n",
      "hidden4/kernel/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias\n",
      "hidden4/bias/Assign\n",
      "hidden4/bias/read\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel\n",
      "hidden5/kernel/Assign\n",
      "hidden5/kernel/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias\n",
      "hidden5/bias/Assign\n",
      "hidden5/bias/read\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "gradients/Shape\n",
      "gradients/Const\n",
      "gradients/Fill\n",
      "gradients/loss/loss_grad/Reshape/shape\n",
      "gradients/loss/loss_grad/Reshape\n",
      "gradients/loss/loss_grad/Shape\n",
      "gradients/loss/loss_grad/Tile\n",
      "gradients/loss/loss_grad/Shape_1\n",
      "gradients/loss/loss_grad/Shape_2\n",
      "gradients/loss/loss_grad/Const\n",
      "gradients/loss/loss_grad/Prod\n",
      "gradients/loss/loss_grad/Const_1\n",
      "gradients/loss/loss_grad/Prod_1\n",
      "gradients/loss/loss_grad/Maximum/y\n",
      "gradients/loss/loss_grad/Maximum\n",
      "gradients/loss/loss_grad/floordiv\n",
      "gradients/loss/loss_grad/Cast\n",
      "gradients/loss/loss_grad/truediv\n",
      "gradients/zeros_like\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value/Minimum/y\n",
      "clip_by_value/Minimum\n",
      "clip_by_value/y\n",
      "clip_by_value\n",
      "clip_by_value_1/Minimum/y\n",
      "clip_by_value_1/Minimum\n",
      "clip_by_value_1/y\n",
      "clip_by_value_1\n",
      "clip_by_value_2/Minimum/y\n",
      "clip_by_value_2/Minimum\n",
      "clip_by_value_2/y\n",
      "clip_by_value_2\n",
      "clip_by_value_3/Minimum/y\n",
      "clip_by_value_3/Minimum\n",
      "clip_by_value_3/y\n",
      "clip_by_value_3\n",
      "clip_by_value_4/Minimum/y\n",
      "clip_by_value_4/Minimum\n",
      "clip_by_value_4/y\n",
      "clip_by_value_4\n",
      "clip_by_value_5/Minimum/y\n",
      "clip_by_value_5/Minimum\n",
      "clip_by_value_5/y\n",
      "clip_by_value_5\n",
      "clip_by_value_6/Minimum/y\n",
      "clip_by_value_6/Minimum\n",
      "clip_by_value_6/y\n",
      "clip_by_value_6\n",
      "clip_by_value_7/Minimum/y\n",
      "clip_by_value_7/Minimum\n",
      "clip_by_value_7/y\n",
      "clip_by_value_7\n",
      "clip_by_value_8/Minimum/y\n",
      "clip_by_value_8/Minimum\n",
      "clip_by_value_8/y\n",
      "clip_by_value_8\n",
      "clip_by_value_9/Minimum/y\n",
      "clip_by_value_9/Minimum\n",
      "clip_by_value_9/y\n",
      "clip_by_value_9\n",
      "clip_by_value_10/Minimum/y\n",
      "clip_by_value_10/Minimum\n",
      "clip_by_value_10/y\n",
      "clip_by_value_10\n",
      "clip_by_value_11/Minimum/y\n",
      "clip_by_value_11/Minimum\n",
      "clip_by_value_11/y\n",
      "clip_by_value_11\n",
      "GradientDescent/learning_rate\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "GradientDescent\n",
      "eval/InTopK\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/accuracy\n",
      "init\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/RestoreV2_1/tensor_names\n",
      "save/RestoreV2_1/shape_and_slices\n",
      "save/RestoreV2_1\n",
      "save/Assign_1\n",
      "save/RestoreV2_2/tensor_names\n",
      "save/RestoreV2_2/shape_and_slices\n",
      "save/RestoreV2_2\n",
      "save/Assign_2\n",
      "save/RestoreV2_3/tensor_names\n",
      "save/RestoreV2_3/shape_and_slices\n",
      "save/RestoreV2_3\n",
      "save/Assign_3\n",
      "save/RestoreV2_4/tensor_names\n",
      "save/RestoreV2_4/shape_and_slices\n",
      "save/RestoreV2_4\n",
      "save/Assign_4\n",
      "save/RestoreV2_5/tensor_names\n",
      "save/RestoreV2_5/shape_and_slices\n",
      "save/RestoreV2_5\n",
      "save/Assign_5\n",
      "save/RestoreV2_6/tensor_names\n",
      "save/RestoreV2_6/shape_and_slices\n",
      "save/RestoreV2_6\n",
      "save/Assign_6\n",
      "save/RestoreV2_7/tensor_names\n",
      "save/RestoreV2_7/shape_and_slices\n",
      "save/RestoreV2_7\n",
      "save/Assign_7\n",
      "save/RestoreV2_8/tensor_names\n",
      "save/RestoreV2_8/shape_and_slices\n",
      "save/RestoreV2_8\n",
      "save/Assign_8\n",
      "save/RestoreV2_9/tensor_names\n",
      "save/RestoreV2_9/shape_and_slices\n",
      "save/RestoreV2_9\n",
      "save/Assign_9\n",
      "save/RestoreV2_10/tensor_names\n",
      "save/RestoreV2_10/shape_and_slices\n",
      "save/RestoreV2_10\n",
      "save/Assign_10\n",
      "save/RestoreV2_11/tensor_names\n",
      "save/RestoreV2_11/shape_and_slices\n",
      "save/RestoreV2_11\n",
      "save/Assign_11\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important_ops\", op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, accuracy, training_op = tf.get_collection(\"my_important_ops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can start a new session to continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9561\n",
      "1 Test accuracy: 0.9581\n",
      "2 Test accuracy: 0.9587\n",
      "3 Test accuracy: 0.9608\n",
      "4 Test accuracy: 0.9591\n",
      "5 Test accuracy: 0.9619\n",
      "6 Test accuracy: 0.9621\n",
      "7 Test accuracy: 0.9639\n",
      "8 Test accuracy: 0.9644\n",
      "9 Test accuracy: 0.9638\n",
      "10 Test accuracy: 0.9648\n",
      "11 Test accuracy: 0.965\n",
      "12 Test accuracy: 0.9652\n",
      "13 Test accuracy: 0.9658\n",
      "14 Test accuracy: 0.9664\n",
      "15 Test accuracy: 0.9656\n",
      "16 Test accuracy: 0.9672\n",
      "17 Test accuracy: 0.9685\n",
      "18 Test accuracy: 0.9694\n",
      "19 Test accuracy: 0.9687\n"
     ]
    }
   ],
   "source": [
    "## saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you have access to the Python code that built the original graph, you can use it instead of import_meta_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9576\n",
      "1 Test accuracy: 0.9584\n",
      "2 Test accuracy: 0.9587\n",
      "3 Test accuracy: 0.9587\n",
      "4 Test accuracy: 0.9617\n",
      "5 Test accuracy: 0.9629\n",
      "6 Test accuracy: 0.9623\n",
      "7 Test accuracy: 0.9555\n",
      "8 Test accuracy: 0.9645\n",
      "9 Test accuracy: 0.9635\n",
      "10 Test accuracy: 0.9642\n",
      "11 Test accuracy: 0.9655\n",
      "12 Test accuracy: 0.9653\n",
      "13 Test accuracy: 0.965\n",
      "14 Test accuracy: 0.9662\n",
      "15 Test accuracy: 0.9666\n",
      "16 Test accuracy: 0.9676\n",
      "17 Test accuracy: 0.968\n",
      "18 Test accuracy: 0.9683\n",
      "19 Test accuracy: 0.9675\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general you will want to reuse only the lower layers. If you are using import_meta_graph() it will load the whole graph, but you can simply ignore the parts you do not need. In this example, we add a new 4th hidden layer on top of the pretrained 3rd layer (ignoring the old 4th hidden layer). We also build a new output layer, the loss for this new output, and a new optimizer to minimize it. We also need another saver to save the whole graph (containing both the entire old graph plus the new operations), and an initialization operation to initialize all the new variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## reuse only the lower layers - tf.get_default_graph().get_tensor_by_name(\"dnn/hidden4/Relu:0\")\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_hidden4 = 20  # new layer\n",
    "n_outputs = 10  # new layer\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden4/Relu:0\")\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n",
    "\n",
    "with tf.name_scope(\"new_loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"new_eval\"):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"new_train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9187\n",
      "1 Test accuracy: 0.938\n",
      "2 Test accuracy: 0.945\n",
      "3 Test accuracy: 0.9463\n",
      "4 Test accuracy: 0.9507\n",
      "5 Test accuracy: 0.9552\n",
      "6 Test accuracy: 0.9568\n",
      "7 Test accuracy: 0.955\n",
      "8 Test accuracy: 0.9574\n",
      "9 Test accuracy: 0.9586\n",
      "10 Test accuracy: 0.9604\n",
      "11 Test accuracy: 0.9614\n",
      "12 Test accuracy: 0.9597\n",
      "13 Test accuracy: 0.9607\n",
      "14 Test accuracy: 0.9644\n",
      "15 Test accuracy: 0.9633\n",
      "16 Test accuracy: 0.9642\n",
      "17 Test accuracy: 0.965\n",
      "18 Test accuracy: 0.9657\n",
      "19 Test accuracy: 0.9665\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have access to the Python code that built the original graph, you can just reuse the parts you need and drop the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.8972\n",
      "1 Test accuracy: 0.9227\n",
      "2 Test accuracy: 0.9312\n",
      "3 Test accuracy: 0.9392\n",
      "4 Test accuracy: 0.9434\n",
      "5 Test accuracy: 0.9473\n",
      "6 Test accuracy: 0.9518\n",
      "7 Test accuracy: 0.9538\n",
      "8 Test accuracy: 0.9555\n",
      "9 Test accuracy: 0.9552\n",
      "10 Test accuracy: 0.9579\n",
      "11 Test accuracy: 0.9581\n",
      "12 Test accuracy: 0.9599\n",
      "13 Test accuracy: 0.9596\n",
      "14 Test accuracy: 0.9612\n",
      "15 Test accuracy: 0.9615\n",
      "16 Test accuracy: 0.9627\n",
      "17 Test accuracy: 0.9632\n",
      "18 Test accuracy: 0.9634\n",
      "19 Test accuracy: 0.9633\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):                                      # not shown in the book\n",
    "        for iteration in range(mnist.train.num_examples // batch_size): # not shown\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)      # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})  # not shown\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,  # not shown\n",
    "                                                y: mnist.test.labels}) # not shown\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)                   # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Models from Other Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  61.   83.  105.]]\n"
     ]
    }
   ],
   "source": [
    "## feed_dict={init_kernel: original_w, init_bias: original_b}\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "    \n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the assignment nodes for the hidden1 variables\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
    "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
    "init_kernel = assign_kernel.inputs[1]\n",
    "init_bias = assign_bias.inputs[1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))  # not shown in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  61.   83.  105.]]\n"
     ]
    }
   ],
   "source": [
    "## feed_dict={init_kernel: original_w, init_bias: original_b}\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the variables of layer hidden1\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # root scope\n",
    "    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n",
    "    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n",
    "\n",
    "# Create dedicated placeholders and assignment nodes\n",
    "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
    "original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n",
    "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n",
    "    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/bias:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/bias:0' shape=(3,) dtype=float32_ref>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing the Lower Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):                                         # not shown in the book\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)     # not shown\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=\"hidden[34]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9174\n",
      "1 Test accuracy: 0.9337\n",
      "2 Test accuracy: 0.9414\n",
      "3 Test accuracy: 0.9445\n",
      "4 Test accuracy: 0.9479\n",
      "5 Test accuracy: 0.9504\n",
      "6 Test accuracy: 0.9507\n",
      "7 Test accuracy: 0.9537\n",
      "8 Test accuracy: 0.953\n",
      "9 Test accuracy: 0.953\n",
      "10 Test accuracy: 0.9549\n",
      "11 Test accuracy: 0.9544\n",
      "12 Test accuracy: 0.955\n",
      "13 Test accuracy: 0.9536\n",
      "14 Test accuracy: 0.9557\n",
      "15 Test accuracy: 0.9554\n",
      "16 Test accuracy: 0.9548\n",
      "17 Test accuracy: 0.9566\n",
      "18 Test accuracy: 0.9564\n",
      "19 Test accuracy: 0.9561\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching the Frozen Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen & cached\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.8728\n",
      "1 Test accuracy: 0.9161\n",
      "2 Test accuracy: 0.9289\n",
      "3 Test accuracy: 0.9351\n",
      "4 Test accuracy: 0.9399\n",
      "5 Test accuracy: 0.9426\n",
      "6 Test accuracy: 0.9448\n",
      "7 Test accuracy: 0.9466\n",
      "8 Test accuracy: 0.9482\n",
      "9 Test accuracy: 0.949\n",
      "10 Test accuracy: 0.9486\n",
      "11 Test accuracy: 0.9508\n",
      "12 Test accuracy: 0.9504\n",
      "13 Test accuracy: 0.9505\n",
      "14 Test accuracy: 0.9517\n",
      "15 Test accuracy: 0.9515\n",
      "16 Test accuracy: 0.9525\n",
      "17 Test accuracy: 0.9522\n",
      "18 Test accuracy: 0.952\n",
      "19 Test accuracy: 0.9526\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_batches = mnist.train.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: mnist.train.images})\n",
    "    h2_cache_test = sess.run(hidden2, feed_dict={X: mnist.test.images}) # not shown in the book\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(mnist.train.num_examples)\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(mnist.train.labels[shuffled_idx], n_batches)\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
    "\n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_test, # not shown\n",
    "                                                y: mnist.test.labels})  # not shown\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)                    # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Momentum optimization\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Nesterov Accelerated Gradient\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## AdaGrad\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## RMSProp\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=0.9, decay=0.9, epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Adam Optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):       # not shown in the book\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9638\n",
      "1 Test accuracy: 0.9729\n",
      "2 Test accuracy: 0.9769\n",
      "3 Test accuracy: 0.9791\n",
      "4 Test accuracy: 0.9793\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001 # l1 regularization hyperparameter\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8148\n",
      "1 Test accuracy: 0.8628\n",
      "2 Test accuracy: 0.8784\n",
      "3 Test accuracy: 0.8863\n",
      "4 Test accuracy: 0.8954\n",
      "5 Test accuracy: 0.8984\n",
      "6 Test accuracy: 0.9013\n",
      "7 Test accuracy: 0.9033\n",
      "8 Test accuracy: 0.9051\n",
      "9 Test accuracy: 0.9064\n",
      "10 Test accuracy: 0.9075\n",
      "11 Test accuracy: 0.9074\n",
      "12 Test accuracy: 0.908\n",
      "13 Test accuracy: 0.9084\n",
      "14 Test accuracy: 0.9074\n",
      "15 Test accuracy: 0.9078\n",
      "16 Test accuracy: 0.9076\n",
      "17 Test accuracy: 0.9069\n",
      "18 Test accuracy: 0.9071\n",
      "19 Test accuracy: 0.9061\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Alternatively\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):                                     # not shown in the book\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # not shown\n",
    "        labels=y, logits=logits)                                # not shown\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   # not shown\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8129\n",
      "1 Test accuracy: 0.8709\n",
      "2 Test accuracy: 0.8896\n",
      "3 Test accuracy: 0.8988\n",
      "4 Test accuracy: 0.9042\n",
      "5 Test accuracy: 0.9095\n",
      "6 Test accuracy: 0.9104\n",
      "7 Test accuracy: 0.9142\n",
      "8 Test accuracy: 0.9156\n",
      "9 Test accuracy: 0.9165\n",
      "10 Test accuracy: 0.9178\n",
      "11 Test accuracy: 0.918\n",
      "12 Test accuracy: 0.9193\n",
      "13 Test accuracy: 0.918\n",
      "14 Test accuracy: 0.9186\n",
      "15 Test accuracy: 0.9178\n",
      "16 Test accuracy: 0.9196\n",
      "17 Test accuracy: 0.9174\n",
      "18 Test accuracy: 0.9167\n",
      "19 Test accuracy: 0.9155\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9172\n",
      "1 Test accuracy: 0.9403\n",
      "2 Test accuracy: 0.9472\n",
      "3 Test accuracy: 0.9529\n",
      "4 Test accuracy: 0.9548\n",
      "5 Test accuracy: 0.9601\n",
      "6 Test accuracy: 0.962\n",
      "7 Test accuracy: 0.9618\n",
      "8 Test accuracy: 0.9609\n",
      "9 Test accuracy: 0.9628\n",
      "10 Test accuracy: 0.964\n",
      "11 Test accuracy: 0.9645\n",
      "12 Test accuracy: 0.9645\n",
      "13 Test accuracy: 0.9678\n",
      "14 Test accuracy: 0.9679\n",
      "15 Test accuracy: 0.964\n",
      "16 Test accuracy: 0.968\n",
      "17 Test accuracy: 0.9708\n",
      "18 Test accuracy: 0.9681\n",
      "19 Test accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)\n",
    "\n",
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9543\n",
      "1 Test accuracy: 0.9642\n",
      "2 Test accuracy: 0.9722\n",
      "3 Test accuracy: 0.975\n",
      "4 Test accuracy: 0.9736\n",
      "5 Test accuracy: 0.9769\n",
      "6 Test accuracy: 0.9804\n",
      "7 Test accuracy: 0.9796\n",
      "8 Test accuracy: 0.979\n",
      "9 Test accuracy: 0.9803\n",
      "10 Test accuracy: 0.9813\n",
      "11 Test accuracy: 0.9804\n",
      "12 Test accuracy: 0.9821\n",
      "13 Test accuracy: 0.9816\n",
      "14 Test accuracy: 0.9828\n",
      "15 Test accuracy: 0.982\n",
      "16 Test accuracy: 0.9826\n",
      "17 Test accuracy: 0.9828\n",
      "18 Test accuracy: 0.9814\n",
      "19 Test accuracy: 0.9829\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:                                              # not shown in the book\n",
    "    init.run()                                                          # not shown\n",
    "    for epoch in range(n_epochs):                                       # not shown\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):  # not shown\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)       # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights.eval()\n",
    "            clip_weights2.eval()                                        # not shown\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,       # not shown\n",
    "                                            y: mnist.test.labels})      # not shown\n",
    "        print(epoch, \"Test accuracy:\", acc_test)                        # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")               # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # there is no regularization loss term\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9486\n",
      "1 Test accuracy: 0.9623\n",
      "2 Test accuracy: 0.9697\n",
      "3 Test accuracy: 0.9726\n",
      "4 Test accuracy: 0.976\n",
      "5 Test accuracy: 0.9761\n",
      "6 Test accuracy: 0.9777\n",
      "7 Test accuracy: 0.978\n",
      "8 Test accuracy: 0.9804\n",
      "9 Test accuracy: 0.9801\n",
      "10 Test accuracy: 0.9803\n",
      "11 Test accuracy: 0.9803\n",
      "12 Test accuracy: 0.9794\n",
      "13 Test accuracy: 0.9796\n",
      "14 Test accuracy: 0.9809\n",
      "15 Test accuracy: 0.9806\n",
      "16 Test accuracy: 0.9816\n",
      "17 Test accuracy: 0.9814\n",
      "18 Test accuracy: 0.9815\n",
      "19 Test accuracy: 0.9815\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,     # not shown in the book\n",
    "                                            y: mnist.test.labels})    # not shown\n",
    "        print(epoch, \"Test accuracy:\", acc_test)                      # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")             # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
