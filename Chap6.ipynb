{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Listing 6.1. Word-level one-hot encoding (toy example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1\n",
    "\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros(shape=(len(samples),\n",
    "                          max_length,\n",
    "                          max(token_index.values()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Listing 6.2. Character-level one-hot encoding (toy example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "characters = string.printable                                           \n",
    "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
    "\n",
    "max_length = 50\n",
    "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50, 101)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "###  Listing 6.3. Using Keras for word-level one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hling/Desktop/Git/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'cat': 2,\n",
       " 'sat': 3,\n",
       " 'on': 4,\n",
       " 'mat': 5,\n",
       " 'dog': 6,\n",
       " 'ate': 7,\n",
       " 'my': 8,\n",
       " 'homework': 9}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Listing 6.4. Word-level one-hot encoding with hashing trick (toy example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "dimensionality = 1000\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Listing 6.5. Instantiating an Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(1000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Listing 6.6. Loading the IMDB data for use with an Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 20)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s 73us/step - loss: 0.6759 - acc: 0.6042 - val_loss: 0.6398 - val_acc: 0.6808\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 2s 78us/step - loss: 0.5658 - acc: 0.7428 - val_loss: 0.5467 - val_acc: 0.7206\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 72us/step - loss: 0.4752 - acc: 0.7808 - val_loss: 0.5113 - val_acc: 0.7384\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 69us/step - loss: 0.4264 - acc: 0.8078 - val_loss: 0.5008 - val_acc: 0.7456\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 69us/step - loss: 0.3930 - acc: 0.8257 - val_loss: 0.4981 - val_acc: 0.7538\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 67us/step - loss: 0.3668 - acc: 0.8395 - val_loss: 0.5013 - val_acc: 0.7532\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s 69us/step - loss: 0.3435 - acc: 0.8533 - val_loss: 0.5051 - val_acc: 0.7516\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 75us/step - loss: 0.3223 - acc: 0.8657 - val_loss: 0.5132 - val_acc: 0.7484\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 67us/step - loss: 0.3023 - acc: 0.8765 - val_loss: 0.5213 - val_acc: 0.7494\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 65us/step - loss: 0.2839 - acc: 0.8860 - val_loss: 0.5302 - val_acc: 0.7466\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Listing 6.8. Processing the labels of the raw IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir = '/Users/hling/Desktop/Git/Python/DeepLearningWithPython/imdb/aclImdb/'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Listing 6.9. Tokenizing the text of the raw IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hling/Desktop/Git/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n",
      "Shape of data tensor: (25000, 100)\n",
      "Shape of label tensor: (25000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Listing 6.10. Parsing the GloVe word-embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = '/Users/hling/Desktop/Git/Python/DeepLearningWithPython/imdb/glove.6B'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Listing 6.11. Preparing the GloVe word-embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Listing 6.12. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Listing 6.13. Loading pretrained word embeddings into the Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Listing 6.14. Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.2617 - acc: 0.4650 - val_loss: 0.8675 - val_acc: 0.4970\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5726 - acc: 0.6500 - val_loss: 1.1370 - val_acc: 0.5082\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4674 - acc: 0.7850 - val_loss: 0.7521 - val_acc: 0.5313\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3019 - acc: 0.9100 - val_loss: 0.7422 - val_acc: 0.5274\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2088 - acc: 0.9800 - val_loss: 0.7969 - val_acc: 0.5301\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1952 - acc: 0.9650 - val_loss: 0.7486 - val_acc: 0.5500\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1991 - acc: 0.9250 - val_loss: 0.9789 - val_acc: 0.5116\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0695 - acc: 1.0000 - val_loss: 0.9131 - val_acc: 0.5336\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0384 - acc: 1.0000 - val_loss: 0.9614 - val_acc: 0.5384\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0212 - acc: 1.0000 - val_loss: 1.4337 - val_acc: 0.5135\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 6.16. Training the same model without pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 200 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6888 - acc: 0.5250 - val_loss: 0.6930 - val_acc: 0.5172\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4743 - acc: 0.9800 - val_loss: 0.7063 - val_acc: 0.5249\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2435 - acc: 0.9800 - val_loss: 0.7245 - val_acc: 0.5153\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1080 - acc: 1.0000 - val_loss: 0.7295 - val_acc: 0.5208\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0497 - acc: 1.0000 - val_loss: 0.7403 - val_acc: 0.5217\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0252 - acc: 1.0000 - val_loss: 0.7612 - val_acc: 0.5232\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0140 - acc: 1.0000 - val_loss: 0.7920 - val_acc: 0.5201\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.7832 - val_acc: 0.5251\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.7948 - val_acc: 0.5257\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.7976 - val_acc: 0.5274\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 6.17. Tokenizing the data of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 62us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4442907068538666, 0.50788]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Listing 6.21. Numpy implementation of a simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "timesteps = 100\n",
    "input_features = 32\n",
    "output_features = 64\n",
    "\n",
    "inputs = np.random.random((timesteps, input_features))\n",
    "\n",
    "state_t = np.zeros((output_features,))\n",
    "\n",
    "W = np.random.random((output_features, input_features))\n",
    "U = np.random.random((output_features, output_features))\n",
    "b = np.random.random((output_features,))\n",
    "\n",
    "successive_outputs = []\n",
    "for input_t in inputs:\n",
    "    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n",
    "\n",
    "    successive_outputs.append(output_t)\n",
    "\n",
    "    state_t = output_t\n",
    "\n",
    "final_output_sequence = np.concatenate(successive_outputs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99999833, 0.99999972, 0.99999918, ..., 1.        , 1.        ,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 6.22. Preparing the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hling/Desktop/Git/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "input_train shape: (25000, 500)\n",
      "input_test shape: (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(\n",
    "     num_words=max_features)\n",
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 6.23. Training the model with Embedding and SimpleRNN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.6474 - acc: 0.6065 - val_loss: 0.7913 - val_acc: 0.5546\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 25s 1ms/step - loss: 0.4142 - acc: 0.8184 - val_loss: 0.6589 - val_acc: 0.6844\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 25s 1ms/step - loss: 0.2966 - acc: 0.8810 - val_loss: 0.3797 - val_acc: 0.8298\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 25s 1ms/step - loss: 0.2237 - acc: 0.9140 - val_loss: 0.4589 - val_acc: 0.7962\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 25s 1ms/step - loss: 0.1635 - acc: 0.9412 - val_loss: 0.3866 - val_acc: 0.8512\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 26s 1ms/step - loss: 0.1138 - acc: 0.9616 - val_loss: 0.4080 - val_acc: 0.8500\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 26s 1ms/step - loss: 0.0733 - acc: 0.9759 - val_loss: 0.5022 - val_acc: 0.8358\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 26s 1ms/step - loss: 0.0472 - acc: 0.9843 - val_loss: 0.5793 - val_acc: 0.8132\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 26s 1ms/step - loss: 0.0293 - acc: 0.9912 - val_loss: 0.7396 - val_acc: 0.7660\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 27s 1ms/step - loss: 0.0177 - acc: 0.9950 - val_loss: 0.6142 - val_acc: 0.8252\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Embedding, SimpleRNN\n",
    "from keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 6.27. Using the LSTM layer in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 83s 4ms/step - loss: 0.5043 - acc: 0.7653 - val_loss: 0.3692 - val_acc: 0.8410\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 84s 4ms/step - loss: 0.2918 - acc: 0.8844 - val_loss: 0.6185 - val_acc: 0.7510\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 84s 4ms/step - loss: 0.2330 - acc: 0.9102 - val_loss: 0.2937 - val_acc: 0.8908\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 81s 4ms/step - loss: 0.1969 - acc: 0.9262 - val_loss: 0.4187 - val_acc: 0.8766\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 79s 4ms/step - loss: 0.1749 - acc: 0.9353 - val_loss: 0.3109 - val_acc: 0.8910\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 79s 4ms/step - loss: 0.1551 - acc: 0.9425 - val_loss: 0.5145 - val_acc: 0.8610\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 78s 4ms/step - loss: 0.1432 - acc: 0.9488 - val_loss: 0.3325 - val_acc: 0.8882\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 78s 4ms/step - loss: 0.1275 - acc: 0.9554 - val_loss: 0.3218 - val_acc: 0.8842\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 78s 4ms/step - loss: 0.1184 - acc: 0.9579 - val_loss: 0.3250 - val_acc: 0.8738\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 79s 4ms/step - loss: 0.1072 - acc: 0.9635 - val_loss: 0.3602 - val_acc: 0.8624\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.28. Inspecting the data of the Jena weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Date Time\"', '\"p (mbar)\"', '\"T (degC)\"', '\"Tpot (K)\"', '\"Tdew (degC)\"', '\"rh (%)\"', '\"VPmax (mbar)\"', '\"VPact (mbar)\"', '\"VPdef (mbar)\"', '\"sh (g/kg)\"', '\"H2OC (mmol/mol)\"', '\"rho (g/m**3)\"', '\"wv (m/s)\"', '\"max. wv (m/s)\"', '\"wd (deg)\"']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '/Users/hling/Desktop/Git/Python/DeepLearningWithPython/jena_climate'\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420551\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(',')[1:]]\n",
    "    float_data[i, :] = values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 6.30. Plotting the temperature timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x123d2a358>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "temp = float_data[:, 1] \n",
    "plt.plot(range(len(temp)), temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12418c438>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1440), temp[:1440])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.32. Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.33. Generator yielding timeseries samples and their targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "              shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.34. Preparing the training, validation, and test generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "train_gen = generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=200000,\n",
    "                      shuffle=True,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "val_gen = generator(float_data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=200001,\n",
    "                    max_index=300000,\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "test_gen = generator(float_data,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=300001,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "val_steps = (300000 - 200001 - lookback)\n",
    "\n",
    "test_steps = (len(float_data) - 300001 - lookback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.35. Computing the common-sense baseline MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28969941979609765\n"
     ]
    }
   ],
   "source": [
    "def evaluate_naive_method():\n",
    "    batch_maes = []\n",
    "    for step in range(val_steps):\n",
    "        samples, targets = next(val_gen)\n",
    "        preds = samples[:, -1, 1]\n",
    "        mae = np.mean(np.abs(preds - targets))\n",
    "        batch_maes.append(mae)\n",
    "    print(np.mean(batch_maes))\n",
    "\n",
    "evaluate_naive_method()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.36. Converting the MAE back to a Celsius error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "celsius_mae = 0.29 * std[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.37. Training and evaluating a densely connected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hling/Desktop/Git/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 863s 2s/step - loss: 1.2115 - val_loss: 0.7032\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 782s 2s/step - loss: 0.4636 - val_loss: 0.3844\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 735s 1s/step - loss: 0.3032 - val_loss: 0.3009\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 854s 2s/step - loss: 0.2666 - val_loss: 0.3101\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 1094s 2s/step - loss: 0.2524 - val_loss: 0.3122\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 895s 2s/step - loss: 0.2426 - val_loss: 0.3111\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 769s 2s/step - loss: 0.2353 - val_loss: 0.3316\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 862s 2s/step - loss: 0.2292 - val_loss: 0.3201\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 810s 2s/step - loss: 0.2245 - val_loss: 0.3264\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 983s 2s/step - loss: 0.2208 - val_loss: 0.3249\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 781s 2s/step - loss: 0.2182 - val_loss: 0.3194\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 826s 2s/step - loss: 0.2148 - val_loss: 0.3291\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 833s 2s/step - loss: 0.2134 - val_loss: 0.3260\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 778s 2s/step - loss: 0.2094 - val_loss: 0.3205\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 776s 2s/step - loss: 0.2066 - val_loss: 0.3496\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 812s 2s/step - loss: 0.2064 - val_loss: 0.3317\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 863s 2s/step - loss: 0.2055 - val_loss: 0.3425\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 845s 2s/step - loss: 0.2016 - val_loss: 0.3386\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 913s 2s/step - loss: 0.2001 - val_loss: 0.3346\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 838s 2s/step - loss: 0.1992 - val_loss: 0.3325\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1])))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.39. Training and evaluating a GRU-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hling/Desktop/Git/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.3084"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.40. Training and evaluating a dropout-regularized GRU-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "                     dropout=0.2,\n",
    "                     recurrent_dropout=0.2,\n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.41. Training and evaluating a dropout-regularized, stacked GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "                     dropout=0.1,\n",
    "                     recurrent_dropout=0.5,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu',\n",
    "                     dropout=0.1,\n",
    "                     recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.42. Training and evaluating an LSTM using reversed sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hling/Desktop/Git/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 135s 7ms/step - loss: 0.5034 - acc: 0.7611 - val_loss: 0.3635 - val_acc: 0.8564\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 129s 6ms/step - loss: 0.3145 - acc: 0.8810 - val_loss: 0.4274 - val_acc: 0.8048\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 117s 6ms/step - loss: 0.2606 - acc: 0.9018 - val_loss: 0.3118 - val_acc: 0.8784\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 124s 6ms/step - loss: 0.2252 - acc: 0.9191 - val_loss: 0.3808 - val_acc: 0.8742\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 145s 7ms/step - loss: 0.1930 - acc: 0.9336 - val_loss: 0.3438 - val_acc: 0.8788\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 119s 6ms/step - loss: 0.1746 - acc: 0.9379 - val_loss: 0.3545 - val_acc: 0.8776\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 110s 5ms/step - loss: 0.1541 - acc: 0.9460 - val_loss: 0.4501 - val_acc: 0.8722\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 117s 6ms/step - loss: 0.1435 - acc: 0.9529 - val_loss: 0.4289 - val_acc: 0.8660\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 124s 6ms/step - loss: 0.1387 - acc: 0.9537 - val_loss: 0.4852 - val_acc: 0.8518\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 122s 6ms/step - loss: 0.1215 - acc: 0.9596 - val_loss: 0.4573 - val_acc: 0.8722\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
    "    num_words=max_features)\n",
    "\n",
    "x_train = [x[::-1] for x in x_train]\n",
    "x_test = [x[::-1] for x in x_test]\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128))\n",
    "model.add(layers.LSTM(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.43. Training and evaluating a bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 123s 6ms/step - loss: 0.5602 - acc: 0.7175 - val_loss: 0.3926 - val_acc: 0.8460\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 123s 6ms/step - loss: 0.3317 - acc: 0.8731 - val_loss: 0.3586 - val_acc: 0.8788\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 129s 6ms/step - loss: 0.2707 - acc: 0.8991 - val_loss: 0.3575 - val_acc: 0.8832\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 125s 6ms/step - loss: 0.2289 - acc: 0.9181 - val_loss: 0.3472 - val_acc: 0.8628\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 128s 6ms/step - loss: 0.2126 - acc: 0.9242 - val_loss: 0.3367 - val_acc: 0.8866\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 128s 6ms/step - loss: 0.1893 - acc: 0.9350 - val_loss: 0.3595 - val_acc: 0.8828\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 132s 7ms/step - loss: 0.1760 - acc: 0.9373 - val_loss: 0.3302 - val_acc: 0.8730\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 115s 6ms/step - loss: 0.1503 - acc: 0.9481 - val_loss: 0.5296 - val_acc: 0.8236\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 115s 6ms/step - loss: 0.1455 - acc: 0.9512 - val_loss: 0.3854 - val_acc: 0.8592\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 115s 6ms/step - loss: 0.1374 - acc: 0.9535 - val_loss: 0.3831 - val_acc: 0.8758\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 32))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.44. Training a bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hling/Desktop/Git/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.2987"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-390d269587f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                               validation_steps=val_steps)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Git/miniconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Git/miniconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1274\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Git/miniconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Git/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2242\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m                                 \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2244\u001b[0;31m                                 max_queue_size=max_queue_size)\n\u001b[0m\u001b[1;32m   2245\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m                             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Git/miniconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Git/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2376\u001b[0m                                      \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2377\u001b[0m                                      str(generator_output))\n\u001b[0;32m-> 2378\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2380\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Git/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1922\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1924\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1925\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Git/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Git/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Git/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Git/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Git/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Git/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Bidirectional(\n",
    "    layers.GRU(32), input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.45. Preparing the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hling/Desktop/Git/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 500)\n",
      "x_test shape: (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "max_len = 500\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.46. Training and evaluating a simple 1D convnet on the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,315,937\n",
      "Trainable params: 1,315,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 65s 3ms/step - loss: 0.8336 - acc: 0.5089 - val_loss: 0.6875 - val_acc: 0.5614\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 63s 3ms/step - loss: 0.6700 - acc: 0.6400 - val_loss: 0.6642 - val_acc: 0.6602\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 67s 3ms/step - loss: 0.6234 - acc: 0.7554 - val_loss: 0.6078 - val_acc: 0.7434\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.5256 - acc: 0.8089 - val_loss: 0.4848 - val_acc: 0.8076\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 68s 3ms/step - loss: 0.4112 - acc: 0.8489 - val_loss: 0.4273 - val_acc: 0.8308\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.3491 - acc: 0.8672 - val_loss: 0.4169 - val_acc: 0.8378\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 67s 3ms/step - loss: 0.3095 - acc: 0.8687 - val_loss: 0.4222 - val_acc: 0.8300\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 78s 4ms/step - loss: 0.2779 - acc: 0.8592 - val_loss: 0.4177 - val_acc: 0.8158\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 79s 4ms/step - loss: 0.2558 - acc: 0.8396 - val_loss: 0.4731 - val_acc: 0.7854\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 63s 3ms/step - loss: 0.2311 - acc: 0.8300 - val_loss: 0.4756 - val_acc: 0.7722\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.47. Training and evaluating a simple 1D convnet on the Jena data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hling/Desktop/Git/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.4175"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 5, activation='relu',\n",
    "                        input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.48. Preparing higher-resolution data generators for the Jena dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 3\n",
    "lookback = 720\n",
    "delay = 144\n",
    "\n",
    "train_gen = generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=200000,\n",
    "                      shuffle=True,\n",
    "                      step=step)\n",
    "val_gen = generator(float_data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=200001,\n",
    "                    max_index=300000,\n",
    "                    step=step)\n",
    "test_gen = generator(float_data,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=300001,\n",
    "                     max_index=None,\n",
    "                     step=step)\n",
    "val_steps = (300000 - 200001 - lookback) // 128\n",
    "test_steps = (len(float_data) - 300001 - lookback) // 128\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.49. Model combining a 1D convolutional base and a GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 5, activation='relu',\n",
    "                        input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
